{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nba_api.stats.endpoints import leaguedashplayerstats\n",
    "from nba_api.stats.endpoints import playerprofilev2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching stats for season 2000-01...\n",
      "Error fetching attributes for player ID 344: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching attributes for player ID 239: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching attributes for player ID 2052: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching attributes for player ID 1051: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching attributes for player ID 1901: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching attributes for player ID 2109: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching attributes for player ID 961: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching attributes for player ID 84: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching attributes for player ID 1718: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching attributes for player ID 895: Expecting value: line 1 column 1 (char 0)\n",
      "Fetching stats for season 2001-02...\n",
      "Error fetching attributes for player ID 239: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching attributes for player ID 2052: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching attributes for player ID 1051: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching attributes for player ID 2109: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching attributes for player ID 1751: HTTPSConnectionPool(host='stats.nba.com', port=443): Read timed out. (read timeout=30)\n",
      "Error fetching attributes for player ID 2054: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Error fetching attributes for player ID 147: HTTPSConnectionPool(host='stats.nba.com', port=443): Read timed out. (read timeout=30)\n",
      "Error fetching attributes for player ID 2048: HTTPSConnectionPool(host='stats.nba.com', port=443): Read timed out. (read timeout=30)\n",
      "Error fetching attributes for player ID 1903: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching attributes for player ID 84: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching attributes for player ID 782: HTTPSConnectionPool(host='stats.nba.com', port=443): Read timed out. (read timeout=30)\n",
      "Error fetching attributes for player ID 397: HTTPSConnectionPool(host='stats.nba.com', port=443): Read timed out. (read timeout=30)\n",
      "Error fetching attributes for player ID 1594: HTTPSConnectionPool(host='stats.nba.com', port=443): Read timed out. (read timeout=30)\n",
      "Error fetching attributes for player ID 70: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching attributes for player ID 2349: HTTPSConnectionPool(host='stats.nba.com', port=443): Read timed out. (read timeout=30)\n",
      "Error fetching attributes for player ID 452: HTTPSConnectionPool(host='stats.nba.com', port=443): Read timed out. (read timeout=30)\n",
      "Error fetching attributes for player ID 1713: HTTPSConnectionPool(host='stats.nba.com', port=443): Read timed out. (read timeout=30)\n",
      "Error fetching attributes for player ID 219: HTTPSConnectionPool(host='stats.nba.com', port=443): Read timed out. (read timeout=30)\n",
      "Error fetching attributes for player ID 958: HTTPSConnectionPool(host='stats.nba.com', port=443): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m start_year \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m\n\u001b[1;32m     78\u001b[0m end_year \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2023\u001b[39m\n\u001b[0;32m---> 79\u001b[0m all_stats \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_player_stats_by_season_with_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_year\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_year\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_stats\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully fetched stats for seasons \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_year\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_year\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 58\u001b[0m, in \u001b[0;36mfetch_player_stats_by_season_with_attributes\u001b[0;34m(start_year, end_year)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Fetch attributes\u001b[39;00m\n\u001b[1;32m     57\u001b[0m player_ids \u001b[38;5;241m=\u001b[39m stats_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPLAYER_ID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m---> 58\u001b[0m attributes \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_player_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplayer_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Map position, height, and weight to the stats DataFrame\u001b[39;00m\n\u001b[1;32m     61\u001b[0m stats_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOSITION\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m stats_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPLAYER_ID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: attributes[x][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPosition\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m, in \u001b[0;36mfetch_player_attributes\u001b[0;34m(player_ids)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m player_id \u001b[38;5;129;01min\u001b[39;00m player_ids:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# Fetch player info\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m         player_info \u001b[38;5;241m=\u001b[39m \u001b[43mcommonplayerinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCommonPlayerInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplayer_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplayer_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         player_info_df \u001b[38;5;241m=\u001b[39m player_info\u001b[38;5;241m.\u001b[39mget_data_frames()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# Extract relevant information\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/nba_api/stats/endpoints/commonplayerinfo.py:75\u001b[0m, in \u001b[0;36mCommonPlayerInfo.__init__\u001b[0;34m(self, player_id, league_id_nullable, proxy, headers, timeout, get_request)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlayerID\u001b[39m\u001b[38;5;124m\"\u001b[39m: player_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeagueID\u001b[39m\u001b[38;5;124m\"\u001b[39m: league_id_nullable}\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_request:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/nba_api/stats/endpoints/commonplayerinfo.py:78\u001b[0m, in \u001b[0;36mCommonPlayerInfo.get_request\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_request\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnba_response \u001b[38;5;241m=\u001b[39m \u001b[43mNBAStatsHTTP\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_api_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_response()\n",
      "File \u001b[0;32m~/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/nba_api/library/http.py:146\u001b[0m, in \u001b[0;36mNBAHTTP.send_api_request\u001b[0;34m(self, endpoint, parameters, referer, proxy, headers, timeout, raise_exception_on_error)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading from file...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m contents:\n\u001b[0;32m--> 146\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     url \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39murl\n\u001b[1;32m    154\u001b[0m     status_code \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code\n",
      "File \u001b[0;32m~/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def fetch_player_attributes(player_ids):\n",
    "    \"\"\"\n",
    "    Fetch player positions, heights, and weights for a list of player IDs.\n",
    "    \"\"\"\n",
    "    attributes = {}\n",
    "    for player_id in player_ids:\n",
    "        try:\n",
    "            player_info = commonplayerinfo.CommonPlayerInfo(player_id=player_id)\n",
    "            player_info_df = player_info.get_data_frames()[0]\n",
    "            \n",
    "            position = player_info_df.get('POSITION', 'N/A')\n",
    "            height = player_info_df.get('HEIGHT', 'N/A')\n",
    "            weight = player_info_df.get('WEIGHT', 'N/A')\n",
    "            \n",
    "            attributes[player_id] = {\n",
    "                'Position': position,\n",
    "                'Height': height,\n",
    "                'Weight': weight\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching attributes for player ID {player_id}: {e}\")\n",
    "            attributes[player_id] = {\n",
    "                'Position': 'N/A',\n",
    "                'Height': 'N/A',\n",
    "                'Weight': 'N/A'\n",
    "            }\n",
    "    \n",
    "    return attributes\n",
    "\n",
    "def fetch_player_stats_by_season_with_attributes(start_year, end_year):\n",
    "    \"\"\"\n",
    "    Fetch all players' stats for each season within the specified range, including player attributes.\n",
    "    \n",
    "    Parameters:\n",
    "        start_year (int): The starting season year (e.g., 2000).\n",
    "        end_year (int): The ending season year (e.g., 2024).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing all player stats for the specified seasons.\n",
    "    \"\"\"\n",
    "    all_seasons_stats = []\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        season = f\"{year}-{str(year + 1)[-2:]}\" \n",
    "        print(f\"Fetching stats for season {season}...\")\n",
    "        \n",
    "        try:\n",
    "            player_stats = leaguedashplayerstats.LeagueDashPlayerStats(season=season)\n",
    "            stats_df = player_stats.get_data_frames()[0] \n",
    "            stats_df['SEASON'] = season\n",
    "\n",
    "            player_ids = stats_df['PLAYER_ID'].unique()\n",
    "            attributes = fetch_player_attributes(player_ids)\n",
    "            \n",
    "            stats_df['POSITION'] = stats_df['PLAYER_ID'].map(lambda x: attributes[x]['Position'])\n",
    "            stats_df['HEIGHT'] = stats_df['PLAYER_ID'].map(lambda x: attributes[x]['Height'])\n",
    "            stats_df['WEIGHT'] = stats_df['PLAYER_ID'].map(lambda x: attributes[x]['Weight'])\n",
    "\n",
    "            all_seasons_stats.append(stats_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch stats for season {season}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if all_seasons_stats:\n",
    "        return pd.concat(all_seasons_stats, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "start_year = 2000\n",
    "end_year = 2023\n",
    "all_stats = fetch_player_stats_by_season_with_attributes(start_year, end_year)\n",
    "\n",
    "if not all_stats.empty:\n",
    "    print(f\"Successfully fetched stats for seasons {start_year} to {end_year}\")\n",
    "    all_stats.to_csv(\"player_stats_with_attributes_2000_2023.csv\", index=False)\n",
    "    print(\"Stats saved to player_stats_with_attributes_2000_2023.csv\")\n",
    "else:\n",
    "    print(\"No data was fetched.\")\n",
    "\n",
    "##Ultimately did not use this code as issues with scraping positions from the NBA_API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PLAYER_ID     PLAYER_NAME NICKNAME     TEAM_ID TEAM_ABBREVIATION   AGE  GP  \\\n",
      "0        920      A.C. Green     A.C.  1610612748               MIA  37.0  82   \n",
      "1       2062     A.J. Guyton     A.J.  1610612741               CHI  23.0  33   \n",
      "2        243     Aaron McKie    Aaron  1610612755               PHI  28.0  76   \n",
      "3       1425  Aaron Williams    Aaron  1610612751               NJN  29.0  82   \n",
      "4        228      Adam Keefe     Adam  1610612744               GSW  31.0  67   \n",
      "\n",
      "    W   L  W_PCT  ...  BLK_RANK  BLKA_RANK  PF_RANK  PFD_RANK  PTS_RANK  \\\n",
      "0  50  32  0.610  ...       271        263      220        39       212   \n",
      "1   6  27  0.182  ...       311         57       92       112       292   \n",
      "2  51  25  0.671  ...       271        376      316       112        87   \n",
      "3  26  56  0.317  ...        24        404      441       112        91   \n",
      "4  14  53  0.209  ...       169        209      200       112       307   \n",
      "\n",
      "   PLUS_MINUS_RANK  NBA_FANTASY_PTS_RANK  DD2_RANK  TD3_RANK  \\\n",
      "0               95                   218       126        26   \n",
      "1              378                   317       224        26   \n",
      "2               20                    84        77         4   \n",
      "3              434                    77        53        26   \n",
      "4              385                   274       224        26   \n",
      "\n",
      "   WNBA_FANTASY_PTS_RANK  \n",
      "0                    223  \n",
      "1                    314  \n",
      "2                     81  \n",
      "3                     79  \n",
      "4                    280  \n",
      "\n",
      "[5 rows x 66 columns]\n"
     ]
    }
   ],
   "source": [
    "player_stats = leaguedashplayerstats.LeagueDashPlayerStats(season='2000-01')\n",
    "stats_df = player_stats.get_data_frames()[0]\n",
    "print(stats_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_position(value):\n",
    "    if isinstance(value, pd.Series):\n",
    "        return value.iloc[0]\n",
    "    return value\n",
    "\n",
    "all_stats['POSITION'] = all_stats['POSITION'].apply(flatten_position)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'str'> <class 'NoneType'>]\n"
     ]
    }
   ],
   "source": [
    "print(all_stats['POSITION'].apply(type).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique positions in the dataset: ['Forward' 'Guard' 'Center-Forward' 'Center' 'Guard-Forward'\n",
      " 'Forward-Center' 'Forward-Guard' '']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLAYER_ID</th>\n",
       "      <th>PLAYER_NAME</th>\n",
       "      <th>NICKNAME</th>\n",
       "      <th>TEAM_ID</th>\n",
       "      <th>TEAM_ABBREVIATION</th>\n",
       "      <th>AGE</th>\n",
       "      <th>GP</th>\n",
       "      <th>W</th>\n",
       "      <th>L</th>\n",
       "      <th>W_PCT</th>\n",
       "      <th>...</th>\n",
       "      <th>PF_RANK</th>\n",
       "      <th>PFD_RANK</th>\n",
       "      <th>PTS_RANK</th>\n",
       "      <th>PLUS_MINUS_RANK</th>\n",
       "      <th>NBA_FANTASY_PTS_RANK</th>\n",
       "      <th>DD2_RANK</th>\n",
       "      <th>TD3_RANK</th>\n",
       "      <th>WNBA_FANTASY_PTS_RANK</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>POSITION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>920</td>\n",
       "      <td>A.C. Green</td>\n",
       "      <td>A.C.</td>\n",
       "      <td>1610612748</td>\n",
       "      <td>MIA</td>\n",
       "      <td>37.0</td>\n",
       "      <td>82</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>0.610</td>\n",
       "      <td>...</td>\n",
       "      <td>220</td>\n",
       "      <td>39</td>\n",
       "      <td>212</td>\n",
       "      <td>95</td>\n",
       "      <td>218</td>\n",
       "      <td>126</td>\n",
       "      <td>26</td>\n",
       "      <td>223</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>Forward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2062</td>\n",
       "      <td>A.J. Guyton</td>\n",
       "      <td>A.J.</td>\n",
       "      <td>1610612741</td>\n",
       "      <td>CHI</td>\n",
       "      <td>23.0</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>0.182</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>112</td>\n",
       "      <td>292</td>\n",
       "      <td>378</td>\n",
       "      <td>317</td>\n",
       "      <td>224</td>\n",
       "      <td>26</td>\n",
       "      <td>314</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>Guard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>243</td>\n",
       "      <td>Aaron McKie</td>\n",
       "      <td>Aaron</td>\n",
       "      <td>1610612755</td>\n",
       "      <td>PHI</td>\n",
       "      <td>28.0</td>\n",
       "      <td>76</td>\n",
       "      <td>51</td>\n",
       "      <td>25</td>\n",
       "      <td>0.671</td>\n",
       "      <td>...</td>\n",
       "      <td>316</td>\n",
       "      <td>112</td>\n",
       "      <td>87</td>\n",
       "      <td>20</td>\n",
       "      <td>84</td>\n",
       "      <td>77</td>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>Guard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1425</td>\n",
       "      <td>Aaron Williams</td>\n",
       "      <td>Aaron</td>\n",
       "      <td>1610612751</td>\n",
       "      <td>NJN</td>\n",
       "      <td>29.0</td>\n",
       "      <td>82</td>\n",
       "      <td>26</td>\n",
       "      <td>56</td>\n",
       "      <td>0.317</td>\n",
       "      <td>...</td>\n",
       "      <td>441</td>\n",
       "      <td>112</td>\n",
       "      <td>91</td>\n",
       "      <td>434</td>\n",
       "      <td>77</td>\n",
       "      <td>53</td>\n",
       "      <td>26</td>\n",
       "      <td>79</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>Center-Forward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>228</td>\n",
       "      <td>Adam Keefe</td>\n",
       "      <td>Adam</td>\n",
       "      <td>1610612744</td>\n",
       "      <td>GSW</td>\n",
       "      <td>31.0</td>\n",
       "      <td>67</td>\n",
       "      <td>14</td>\n",
       "      <td>53</td>\n",
       "      <td>0.209</td>\n",
       "      <td>...</td>\n",
       "      <td>200</td>\n",
       "      <td>112</td>\n",
       "      <td>307</td>\n",
       "      <td>385</td>\n",
       "      <td>274</td>\n",
       "      <td>224</td>\n",
       "      <td>26</td>\n",
       "      <td>280</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>Forward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12152</th>\n",
       "      <td>1641744</td>\n",
       "      <td>Zach Edey</td>\n",
       "      <td>Zach</td>\n",
       "      <td>1610612763</td>\n",
       "      <td>MEM</td>\n",
       "      <td>22.0</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.571</td>\n",
       "      <td>...</td>\n",
       "      <td>385</td>\n",
       "      <td>155</td>\n",
       "      <td>199</td>\n",
       "      <td>91</td>\n",
       "      <td>203</td>\n",
       "      <td>67</td>\n",
       "      <td>20</td>\n",
       "      <td>211</td>\n",
       "      <td>2024-25</td>\n",
       "      <td>Center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12153</th>\n",
       "      <td>203897</td>\n",
       "      <td>Zach LaVine</td>\n",
       "      <td>Zach</td>\n",
       "      <td>1610612741</td>\n",
       "      <td>CHI</td>\n",
       "      <td>29.0</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>0.476</td>\n",
       "      <td>...</td>\n",
       "      <td>339</td>\n",
       "      <td>68</td>\n",
       "      <td>28</td>\n",
       "      <td>216</td>\n",
       "      <td>50</td>\n",
       "      <td>91</td>\n",
       "      <td>20</td>\n",
       "      <td>38</td>\n",
       "      <td>2024-25</td>\n",
       "      <td>Guard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12154</th>\n",
       "      <td>1630192</td>\n",
       "      <td>Zeke Nnaji</td>\n",
       "      <td>Zeke</td>\n",
       "      <td>1610612743</td>\n",
       "      <td>DEN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.643</td>\n",
       "      <td>...</td>\n",
       "      <td>102</td>\n",
       "      <td>343</td>\n",
       "      <td>405</td>\n",
       "      <td>439</td>\n",
       "      <td>408</td>\n",
       "      <td>146</td>\n",
       "      <td>20</td>\n",
       "      <td>412</td>\n",
       "      <td>2024-25</td>\n",
       "      <td>Forward-Center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12155</th>\n",
       "      <td>1630533</td>\n",
       "      <td>Ziaire Williams</td>\n",
       "      <td>Ziaire</td>\n",
       "      <td>1610612751</td>\n",
       "      <td>BKN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0.429</td>\n",
       "      <td>...</td>\n",
       "      <td>456</td>\n",
       "      <td>108</td>\n",
       "      <td>173</td>\n",
       "      <td>436</td>\n",
       "      <td>177</td>\n",
       "      <td>91</td>\n",
       "      <td>20</td>\n",
       "      <td>180</td>\n",
       "      <td>2024-25</td>\n",
       "      <td>Forward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12156</th>\n",
       "      <td>1629627</td>\n",
       "      <td>Zion Williamson</td>\n",
       "      <td>Zion</td>\n",
       "      <td>1610612740</td>\n",
       "      <td>NOP</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.333</td>\n",
       "      <td>...</td>\n",
       "      <td>194</td>\n",
       "      <td>117</td>\n",
       "      <td>226</td>\n",
       "      <td>310</td>\n",
       "      <td>252</td>\n",
       "      <td>54</td>\n",
       "      <td>20</td>\n",
       "      <td>250</td>\n",
       "      <td>2024-25</td>\n",
       "      <td>Forward</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12157 rows Ã— 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PLAYER_ID      PLAYER_NAME NICKNAME     TEAM_ID TEAM_ABBREVIATION  \\\n",
       "0            920       A.C. Green     A.C.  1610612748               MIA   \n",
       "1           2062      A.J. Guyton     A.J.  1610612741               CHI   \n",
       "2            243      Aaron McKie    Aaron  1610612755               PHI   \n",
       "3           1425   Aaron Williams    Aaron  1610612751               NJN   \n",
       "4            228       Adam Keefe     Adam  1610612744               GSW   \n",
       "...          ...              ...      ...         ...               ...   \n",
       "12152    1641744        Zach Edey     Zach  1610612763               MEM   \n",
       "12153     203897      Zach LaVine     Zach  1610612741               CHI   \n",
       "12154    1630192       Zeke Nnaji     Zeke  1610612743               DEN   \n",
       "12155    1630533  Ziaire Williams   Ziaire  1610612751               BKN   \n",
       "12156    1629627  Zion Williamson     Zion  1610612740               NOP   \n",
       "\n",
       "        AGE  GP   W   L  W_PCT  ...  PF_RANK  PFD_RANK  PTS_RANK  \\\n",
       "0      37.0  82  50  32  0.610  ...      220        39       212   \n",
       "1      23.0  33   6  27  0.182  ...       92       112       292   \n",
       "2      28.0  76  51  25  0.671  ...      316       112        87   \n",
       "3      29.0  82  26  56  0.317  ...      441       112        91   \n",
       "4      31.0  67  14  53  0.209  ...      200       112       307   \n",
       "...     ...  ..  ..  ..    ...  ...      ...       ...       ...   \n",
       "12152  22.0  14   8   6  0.571  ...      385       155       199   \n",
       "12153  29.0  21  10  11  0.476  ...      339        68        28   \n",
       "12154  23.0  14   9   5  0.643  ...      102       343       405   \n",
       "12155  23.0  21   9  12  0.429  ...      456       108       173   \n",
       "12156  24.0   6   2   4  0.333  ...      194       117       226   \n",
       "\n",
       "       PLUS_MINUS_RANK  NBA_FANTASY_PTS_RANK  DD2_RANK  TD3_RANK  \\\n",
       "0                   95                   218       126        26   \n",
       "1                  378                   317       224        26   \n",
       "2                   20                    84        77         4   \n",
       "3                  434                    77        53        26   \n",
       "4                  385                   274       224        26   \n",
       "...                ...                   ...       ...       ...   \n",
       "12152               91                   203        67        20   \n",
       "12153              216                    50        91        20   \n",
       "12154              439                   408       146        20   \n",
       "12155              436                   177        91        20   \n",
       "12156              310                   252        54        20   \n",
       "\n",
       "       WNBA_FANTASY_PTS_RANK   SEASON        POSITION  \n",
       "0                        223  2000-01         Forward  \n",
       "1                        314  2000-01           Guard  \n",
       "2                         81  2000-01           Guard  \n",
       "3                         79  2000-01  Center-Forward  \n",
       "4                        280  2000-01         Forward  \n",
       "...                      ...      ...             ...  \n",
       "12152                    211  2024-25          Center  \n",
       "12153                     38  2024-25           Guard  \n",
       "12154                    412  2024-25  Forward-Center  \n",
       "12155                    180  2024-25         Forward  \n",
       "12156                    250  2024-25         Forward  \n",
       "\n",
       "[12157 rows x 68 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_positions = all_stats['POSITION'].dropna().unique()\n",
    "print(\"Unique positions in the dataset:\", unique_positions)\n",
    "\n",
    "all_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2001_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2001_shooting.html\n",
      "Parsing HTML file: NBA_2001_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2001_shooting.csv\n"
     ]
    }
   ],
   "source": [
    "#New code to scrape the basketballreference website\n",
    "\n",
    "def download_webpage(url, output_file):\n",
    "    \"\"\"\n",
    "    Downloads a webpage and saves it locally.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading data from: {url}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        \n",
    "        print(f\"Webpage successfully downloaded and saved as {output_file}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while downloading the webpage: {e}\")\n",
    "\n",
    "def html_to_csv(input_file, output_csv):\n",
    "    \"\"\"\n",
    "    Converts an HTML file to a CSV by extracting all rows in the table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Parsing HTML file: {input_file}\")\n",
    "\n",
    "        with open(input_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'html.parser')\n",
    "        \n",
    "        table = soup.find('table', {'id': 'shooting'})\n",
    "        if not table:\n",
    "            print(\"Shooting table not found in the HTML file.\")\n",
    "            return\n",
    "        \n",
    "        headers = [th.get_text(strip=True) for th in table.find('thead').find_all('tr')[-1].find_all('th')]\n",
    "\n",
    "        rows = table.find('tbody').find_all('tr')\n",
    "        data = []\n",
    "        for row in rows:\n",
    "            if row.find('th', {\"scope\": \"row\"}): \n",
    "                row_data = [td.get_text(strip=True) for td in row.find_all('td')]\n",
    "                row_index = row.find('th').get_text(strip=True) \n",
    "                full_row = [row_index] + row_data\n",
    "                \n",
    "                if len(full_row) < len(headers):\n",
    "                    full_row.extend([''] * (len(headers) - len(full_row))) \n",
    "                elif len(full_row) > len(headers):\n",
    "                    full_row = full_row[:len(headers)] \n",
    "                \n",
    "                data.append(full_row)\n",
    "        \n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"Data successfully converted to CSV and saved as {output_csv}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while converting HTML to CSV: {e}\")\n",
    "\n",
    "url = \"https://www.basketball-reference.com/leagues/NBA_2001_shooting.html#shooting\"\n",
    "html_file = \"NBA_2001_shooting.html\"\n",
    "csv_file = \"NBA_2001_shooting.csv\"\n",
    "\n",
    "download_webpage(url, html_file)\n",
    "html_to_csv(html_file, csv_file)\n",
    "\n",
    "#This was successfull so then I iterate it in the next code chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2001_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2001_shooting.html\n",
      "Parsing HTML file: NBA_2001_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2001_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2002_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2002_shooting.html\n",
      "Parsing HTML file: NBA_2002_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2002_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2003_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2003_shooting.html\n",
      "Parsing HTML file: NBA_2003_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2003_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2004_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2004_shooting.html\n",
      "Parsing HTML file: NBA_2004_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2004_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2005_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2005_shooting.html\n",
      "Parsing HTML file: NBA_2005_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2005_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2006_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2006_shooting.html\n",
      "Parsing HTML file: NBA_2006_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2006_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2007_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2007_shooting.html\n",
      "Parsing HTML file: NBA_2007_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2007_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2008_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2008_shooting.html\n",
      "Parsing HTML file: NBA_2008_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2008_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2009_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2009_shooting.html\n",
      "Parsing HTML file: NBA_2009_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2009_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2010_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2010_shooting.html\n",
      "Parsing HTML file: NBA_2010_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2010_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2011_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2011_shooting.html\n",
      "Parsing HTML file: NBA_2011_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2011_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2012_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2012_shooting.html\n",
      "Parsing HTML file: NBA_2012_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2012_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2013_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2013_shooting.html\n",
      "Parsing HTML file: NBA_2013_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2013_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2014_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2014_shooting.html\n",
      "Parsing HTML file: NBA_2014_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2014_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2015_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2015_shooting.html\n",
      "Parsing HTML file: NBA_2015_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2015_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2016_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2016_shooting.html\n",
      "Parsing HTML file: NBA_2016_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2016_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2017_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2017_shooting.html\n",
      "Parsing HTML file: NBA_2017_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2017_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2018_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2018_shooting.html\n",
      "Parsing HTML file: NBA_2018_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2018_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2019_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2019_shooting.html\n",
      "Parsing HTML file: NBA_2019_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2019_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2020_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2020_shooting.html\n",
      "Parsing HTML file: NBA_2020_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2020_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2021_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2021_shooting.html\n",
      "Parsing HTML file: NBA_2021_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2021_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2022_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2022_shooting.html\n",
      "Parsing HTML file: NBA_2022_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2022_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2023_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2023_shooting.html\n",
      "Parsing HTML file: NBA_2023_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2023_shooting.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2024_shooting.html#shooting\n",
      "Webpage successfully downloaded and saved as NBA_2024_shooting.html\n",
      "Parsing HTML file: NBA_2024_shooting.html\n",
      "Data successfully converted to CSV and saved as NBA_2024_shooting.csv\n"
     ]
    }
   ],
   "source": [
    "def download_webpage(url, output_file):\n",
    "    \"\"\"\n",
    "    Downloads a webpage and saves it locally.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading data from: {url}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        \n",
    "        print(f\"Webpage successfully downloaded and saved as {output_file}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while downloading the webpage: {e}\")\n",
    "\n",
    "def html_to_csv(input_file, output_csv):\n",
    "    \"\"\"\n",
    "    Converts an HTML file to a CSV by extracting all rows in the table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Parsing HTML file: {input_file}\")\n",
    "        \n",
    "        with open(input_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'html.parser')\n",
    "        \n",
    "        table = soup.find('table', {'id': 'shooting'})\n",
    "        if not table:\n",
    "            print(\"Shooting table not found in the HTML file.\")\n",
    "            return\n",
    "        \n",
    "        headers = [th.get_text(strip=True) for th in table.find('thead').find_all('tr')[-1].find_all('th')]\n",
    "\n",
    "        rows = table.find('tbody').find_all('tr')\n",
    "        data = []\n",
    "        for row in rows:\n",
    "            if row.find('th', {\"scope\": \"row\"}): \n",
    "                row_data = [td.get_text(strip=True) for td in row.find_all('td')]\n",
    "                row_index = row.find('th').get_text(strip=True) \n",
    "                full_row = [row_index] + row_data\n",
    "                \n",
    "                if len(full_row) < len(headers):\n",
    "                    full_row.extend([''] * (len(headers) - len(full_row)))\n",
    "                elif len(full_row) > len(headers):\n",
    "                    full_row = full_row[:len(headers)]\n",
    "                \n",
    "                data.append(full_row)\n",
    "        \n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"Data successfully converted to CSV and saved as {output_csv}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while converting HTML to CSV: {e}\")\n",
    "\n",
    "def scrape_seasons(start_year, end_year):\n",
    "    \"\"\"\n",
    "    Scrapes data for each season from start_year to end_year.\n",
    "    \"\"\"\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        url = f\"https://www.basketball-reference.com/leagues/NBA_{year}_shooting.html#shooting\"\n",
    "        html_file = f\"NBA_{year}_shooting.html\"\n",
    "        csv_file = f\"NBA_{year}_shooting.csv\"\n",
    "        \n",
    "        download_webpage(url, html_file)\n",
    "        html_to_csv(html_file, csv_file)\n",
    "\n",
    "scrape_seasons(2001, 2024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NBA_2010_shooting.csv...\n",
      "Loading NBA_2017_shooting.csv...\n",
      "Loading NBA_2002_shooting.csv...\n",
      "Loading NBA_2005_shooting.csv...\n",
      "Loading NBA_2021_shooting.csv...\n",
      "Loading NBA_2004_shooting.csv...\n",
      "Loading NBA_2003_shooting.csv...\n",
      "Loading NBA_2016_shooting.csv...\n",
      "Loading NBA_2011_shooting.csv...\n",
      "Loading NBA_2020_shooting.csv...\n",
      "Loading NBA_2009_shooting.csv...\n",
      "Loading NBA_2008_shooting.csv...\n",
      "Loading NBA_2018_shooting.csv...\n",
      "Loading NBA_2019_shooting.csv...\n",
      "Loading NBA_2001_shooting.csv...\n",
      "Loading NBA_2006_shooting.csv...\n",
      "Loading NBA_2013_shooting.csv...\n",
      "Loading NBA_2014_shooting.csv...\n",
      "Loading NBA_2022_shooting.csv...\n",
      "Loading NBA_2015_shooting.csv...\n",
      "Loading NBA_2012_shooting.csv...\n",
      "Loading NBA_2007_shooting.csv...\n",
      "Loading NBA_2024_shooting.csv...\n",
      "Loading NBA_2023_shooting.csv...\n",
      "All CSV files merged successfully into /Users/nicholasrichards/Desktop/nba_shootingstats_merged.csv\n"
     ]
    }
   ],
   "source": [
    "#Merges the individual csvs into one csv\n",
    "\n",
    "def merge_csv_files(directory, output_file):\n",
    "    \"\"\"\n",
    "    Merges all CSV files in the specified directory into a single CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "        \n",
    "\n",
    "        all_data = []\n",
    "\n",
    "        for csv_file in csv_files:\n",
    "            file_path = os.path.join(directory, csv_file)\n",
    "            print(f\"Loading {csv_file}...\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            season = csv_file.split('_')[1] \n",
    "            df['Season'] = season\n",
    "            all_data.append(df)\n",
    "\n",
    "        merged_data = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        merged_data.to_csv(output_file, index=False)\n",
    "        print(f\"All CSV files merged successfully into {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error while merging CSV files: {e}\")\n",
    "\n",
    "directory = '/Users/nicholasrichards/Desktop/nba_shootingstats_csvs'\n",
    "output_file = '/Users/nicholasrichards/Desktop/nba_shootingstats_merged.csv'\n",
    "\n",
    "merge_csv_files(directory, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed players with MP less than 1000. Remaining players: 6600\n",
      "Updated CSV saved as /Users/nicholasrichards/Desktop/nba_shootingstats_csvs/nba_shootingstats_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_players_with_low_mp(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Removes players with MP (minutes played) less than 1000 from the CSV file and saves the updated file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "\n",
    "        if 'MP' in df.columns:\n",
    "            df_filtered = df[df['MP'] >= 1000]\n",
    "            print(f\"Removed players with MP less than 1000. Remaining players: {len(df_filtered)}\")\n",
    "        else:\n",
    "            print(\"'MP' column not found in the file.\")\n",
    "            return\n",
    "        \n",
    "        df_filtered.to_csv(output_file, index=False)\n",
    "        print(f\"Updated CSV saved as {output_file}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error while removing players with low MP: {e}\")\n",
    "\n",
    "input_file = '/Users/nicholasrichards/Desktop/nba_shootingstats_csvs/nba_shootingstats_cleaned.csv'\n",
    "output_file = '/Users/nicholasrichards/Desktop/nba_shootingstats_csvs/nba_shootingstats_cleaned.csv'\n",
    "\n",
    "remove_players_with_low_mp(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CSV file has 6600 rows.\n"
     ]
    }
   ],
   "source": [
    "def report_rows_in_csv(file_path):\n",
    "    \"\"\"\n",
    "    Loads the CSV file and reports the number of rows.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        num_rows = len(df)\n",
    "        print(f\"The CSV file has {num_rows} rows.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error while loading the CSV file: {e}\")\n",
    "\n",
    "file_path = '/Users/nicholasrichards/Desktop/nba_shootingstats_csvs/nba_shootingstats_cleaned.csv'\n",
    "\n",
    "report_rows_in_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2001_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2001_totals.html\n",
      "Parsing HTML file: NBA_2001_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2001_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2002_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2002_totals.html\n",
      "Parsing HTML file: NBA_2002_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2002_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2003_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2003_totals.html\n",
      "Parsing HTML file: NBA_2003_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2003_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2004_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2004_totals.html\n",
      "Parsing HTML file: NBA_2004_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2004_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2005_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2005_totals.html\n",
      "Parsing HTML file: NBA_2005_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2005_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2006_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2006_totals.html\n",
      "Parsing HTML file: NBA_2006_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2006_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2007_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2007_totals.html\n",
      "Parsing HTML file: NBA_2007_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2007_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2008_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2008_totals.html\n",
      "Parsing HTML file: NBA_2008_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2008_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2009_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2009_totals.html\n",
      "Parsing HTML file: NBA_2009_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2009_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2010_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2010_totals.html\n",
      "Parsing HTML file: NBA_2010_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2010_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2011_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2011_totals.html\n",
      "Parsing HTML file: NBA_2011_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2011_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2012_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2012_totals.html\n",
      "Parsing HTML file: NBA_2012_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2012_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2013_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2013_totals.html\n",
      "Parsing HTML file: NBA_2013_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2013_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2014_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2014_totals.html\n",
      "Parsing HTML file: NBA_2014_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2014_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2015_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2015_totals.html\n",
      "Parsing HTML file: NBA_2015_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2015_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2016_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2016_totals.html\n",
      "Parsing HTML file: NBA_2016_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2016_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2017_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2017_totals.html\n",
      "Parsing HTML file: NBA_2017_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2017_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2018_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2018_totals.html\n",
      "Parsing HTML file: NBA_2018_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2018_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2019_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2019_totals.html\n",
      "Parsing HTML file: NBA_2019_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2019_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2020_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2020_totals.html\n",
      "Parsing HTML file: NBA_2020_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2020_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2021_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2021_totals.html\n",
      "Parsing HTML file: NBA_2021_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2021_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2022_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2022_totals.html\n",
      "Parsing HTML file: NBA_2022_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2022_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2023_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2023_totals.html\n",
      "Parsing HTML file: NBA_2023_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2023_totals.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2024_totals.html#totals_stats\n",
      "Webpage successfully downloaded and saved as NBA_2024_totals.html\n",
      "Parsing HTML file: NBA_2024_totals.html\n",
      "Data successfully converted to CSV and saved as NBA_2024_totals.csv\n"
     ]
    }
   ],
   "source": [
    "#This is the same as the previous webscraper for basketball reference but now scraping for total stats not just shooting stats\n",
    "\n",
    "#There was probably an easier way to scrape each url but pressed for time so did it this way\n",
    "\n",
    "def download_webpage(url, output_file):\n",
    "    \"\"\"\n",
    "    Downloads a webpage and saves it locally.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading data from: {url}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        \n",
    "        print(f\"Webpage successfully downloaded and saved as {output_file}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while downloading the webpage: {e}\")\n",
    "\n",
    "def html_to_csv(input_file, output_csv):\n",
    "    \"\"\"\n",
    "    Converts an HTML file to a CSV by extracting all rows in the totals table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Parsing HTML file: {input_file}\")\n",
    "        \n",
    "        with open(input_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "        table = soup.find('table', {'id': 'totals_stats'})\n",
    "        if not table:\n",
    "            print(\"Totals table not found in the HTML file.\")\n",
    "            return\n",
    "        \n",
    "        headers = [th.get_text(strip=True) for th in table.find('thead').find_all('tr')[-1].find_all('th')]\n",
    "\n",
    "        rows = table.find('tbody').find_all('tr')\n",
    "        data = []\n",
    "        for row in rows:\n",
    "            if row.find('th', {\"scope\": \"row\"}):\n",
    "                row_data = [td.get_text(strip=True) for td in row.find_all('td')]\n",
    "                row_index = row.find('th').get_text(strip=True)\n",
    "                full_row = [row_index] + row_data\n",
    "\n",
    "                if len(full_row) < len(headers):\n",
    "                    full_row.extend([''] * (len(headers) - len(full_row))) \n",
    "                elif len(full_row) > len(headers):\n",
    "                    full_row = full_row[:len(headers)]\n",
    "                \n",
    "                data.append(full_row)\n",
    "        \n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"Data successfully converted to CSV and saved as {output_csv}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while converting HTML to CSV: {e}\")\n",
    "\n",
    "def scrape_seasons(start_year, end_year):\n",
    "    \"\"\"\n",
    "    Scrapes data for each season from start_year to end_year.\n",
    "    \"\"\"\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        url = f\"https://www.basketball-reference.com/leagues/NBA_{year}_totals.html#totals_stats\"\n",
    "        html_file = f\"NBA_{year}_totals.html\"\n",
    "        csv_file = f\"NBA_{year}_totals.csv\"\n",
    "        \n",
    "        download_webpage(url, html_file)\n",
    "        html_to_csv(html_file, csv_file)\n",
    "\n",
    "scrape_seasons(2001, 2024)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NBA_2013_totals.csv...\n",
      "Loading NBA_2024_totals.csv...\n",
      "Loading NBA_2002_totals.csv...\n",
      "Loading NBA_2007_totals.csv...\n",
      "Loading NBA_2021_totals.csv...\n",
      "Loading NBA_2016_totals.csv...\n",
      "Loading NBA_2011_totals.csv...\n",
      "Loading NBA_2005_totals.csv...\n",
      "Loading NBA_2019_totals.csv...\n",
      "Loading NBA_2008_totals.csv...\n",
      "Loading NBA_2014_totals.csv...\n",
      "Loading NBA_2023_totals.csv...\n",
      "Loading NBA_2006_totals.csv...\n",
      "Loading NBA_2020_totals.csv...\n",
      "Loading NBA_2017_totals.csv...\n",
      "Loading NBA_2012_totals.csv...\n",
      "Loading NBA_2003_totals.csv...\n",
      "Loading NBA_2004_totals.csv...\n",
      "Loading NBA_2018_totals.csv...\n",
      "Loading NBA_2009_totals.csv...\n",
      "Loading NBA_2015_totals.csv...\n",
      "Loading NBA_2022_totals.csv...\n",
      "Loading NBA_2010_totals.csv...\n",
      "Loading NBA_2001_totals.csv...\n",
      "All CSV files merged successfully into /Users/nicholasrichards/Desktop/nba_totalstats_csvs/nba_totalstats_merged.csv\n"
     ]
    }
   ],
   "source": [
    "def merge_csv_files(directory, output_file):\n",
    "    \"\"\"\n",
    "    Merges all CSV files in the specified directory into a single CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "        \n",
    "        all_data = []\n",
    "\n",
    "        for csv_file in csv_files:\n",
    "            file_path = os.path.join(directory, csv_file)\n",
    "            print(f\"Loading {csv_file}...\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            season = csv_file.split('_')[1] \n",
    "            df['Season'] = season\n",
    "            all_data.append(df)\n",
    "\n",
    "        merged_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "        merged_data.to_csv(output_file, index=False)\n",
    "        print(f\"All CSV files merged successfully into {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error while merging CSV files: {e}\")\n",
    "\n",
    "directory = '/Users/nicholasrichards/Desktop/nba_totalstats_csvs'\n",
    "output_file = '/Users/nicholasrichards/Desktop/nba_totalstats_csvs/nba_totalstats_merged.csv'\n",
    "\n",
    "merge_csv_files(directory, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file sorted by season and saved as /Users/nicholasrichards/Desktop/nba_totalstats_csvs/nba_totalstats_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "def sort_csv_by_season(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Sorts the CSV file by the 'Season' column, from 2001 to 2024.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "\n",
    "        df['Season'] = pd.to_numeric(df['Season'], errors='coerce')\n",
    "\n",
    "        df_sorted = df.sort_values(by='Season', ascending=True)\n",
    "\n",
    "        df_sorted.to_csv(output_file, index=False)\n",
    "        print(f\"CSV file sorted by season and saved as {output_file}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error while sorting the CSV: {e}\")\n",
    "\n",
    "input_file = '/Users/nicholasrichards/Desktop/nba_totalstats_csvs/nba_totalstats_cleaned.csv'\n",
    "output_file = '/Users/nicholasrichards/Desktop/nba_totalstats_csvs/nba_totalstats_cleaned.csv'\n",
    "\n",
    "sort_csv_by_season(input_file, output_file)\n",
    "\n",
    "#There is some cleaning code that I wrote over in the chunk but all of the cleaning I did for the shooting stats data was done for the totals and advanced stats data as well\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2001_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2001_advanced.html\n",
      "Parsing HTML file: NBA_2001_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2001_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2002_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2002_advanced.html\n",
      "Parsing HTML file: NBA_2002_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2002_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2003_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2003_advanced.html\n",
      "Parsing HTML file: NBA_2003_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2003_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2004_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2004_advanced.html\n",
      "Parsing HTML file: NBA_2004_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2004_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2005_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2005_advanced.html\n",
      "Parsing HTML file: NBA_2005_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2005_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2006_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2006_advanced.html\n",
      "Parsing HTML file: NBA_2006_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2006_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2007_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2007_advanced.html\n",
      "Parsing HTML file: NBA_2007_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2007_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2008_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2008_advanced.html\n",
      "Parsing HTML file: NBA_2008_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2008_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2009_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2009_advanced.html\n",
      "Parsing HTML file: NBA_2009_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2009_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2010_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2010_advanced.html\n",
      "Parsing HTML file: NBA_2010_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2010_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2011_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2011_advanced.html\n",
      "Parsing HTML file: NBA_2011_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2011_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2012_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2012_advanced.html\n",
      "Parsing HTML file: NBA_2012_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2012_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2013_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2013_advanced.html\n",
      "Parsing HTML file: NBA_2013_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2013_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2014_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2014_advanced.html\n",
      "Parsing HTML file: NBA_2014_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2014_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2015_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2015_advanced.html\n",
      "Parsing HTML file: NBA_2015_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2015_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2016_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2016_advanced.html\n",
      "Parsing HTML file: NBA_2016_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2016_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2017_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2017_advanced.html\n",
      "Parsing HTML file: NBA_2017_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2017_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2018_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2018_advanced.html\n",
      "Parsing HTML file: NBA_2018_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2018_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2019_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2019_advanced.html\n",
      "Parsing HTML file: NBA_2019_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2019_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2020_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2020_advanced.html\n",
      "Parsing HTML file: NBA_2020_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2020_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2021_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2021_advanced.html\n",
      "Parsing HTML file: NBA_2021_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2021_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2022_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2022_advanced.html\n",
      "Parsing HTML file: NBA_2022_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2022_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2023_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2023_advanced.html\n",
      "Parsing HTML file: NBA_2023_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2023_advanced.csv\n",
      "Downloading data from: https://www.basketball-reference.com/leagues/NBA_2024_advanced.html#advanced\n",
      "Webpage successfully downloaded and saved as NBA_2024_advanced.html\n",
      "Parsing HTML file: NBA_2024_advanced.html\n",
      "Data successfully converted to CSV and saved as NBA_2024_advanced.csv\n"
     ]
    }
   ],
   "source": [
    "def download_webpage(url, output_file):\n",
    "    \"\"\"\n",
    "    Downloads a webpage and saves it locally.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading data from: {url}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        \n",
    "        print(f\"Webpage successfully downloaded and saved as {output_file}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while downloading the webpage: {e}\")\n",
    "\n",
    "def html_to_csv(input_file, output_csv):\n",
    "    \"\"\"\n",
    "    Converts an HTML file to a CSV by extracting all rows in the totals table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Parsing HTML file: {input_file}\")\n",
    "        \n",
    "        with open(input_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'html.parser')\n",
    "        \n",
    "        table = soup.find('table', {'id': 'advanced'})\n",
    "        if not table:\n",
    "            print(\"Advanced table not found in the HTML file.\")\n",
    "            return\n",
    "        \n",
    "        headers = [th.get_text(strip=True) for th in table.find('thead').find_all('tr')[-1].find_all('th')]\n",
    "\n",
    "        rows = table.find('tbody').find_all('tr')\n",
    "        data = []\n",
    "        for row in rows:\n",
    "            if row.find('th', {\"scope\": \"row\"}):\n",
    "                row_data = [td.get_text(strip=True) for td in row.find_all('td')]\n",
    "                row_index = row.find('th').get_text(strip=True)\n",
    "                full_row = [row_index] + row_data\n",
    "\n",
    "                if len(full_row) < len(headers):\n",
    "                    full_row.extend([''] * (len(headers) - len(full_row))) \n",
    "                elif len(full_row) > len(headers):\n",
    "                    full_row = full_row[:len(headers)]\n",
    "                \n",
    "                data.append(full_row)\n",
    "\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"Data successfully converted to CSV and saved as {output_csv}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while converting HTML to CSV: {e}\")\n",
    "\n",
    "def scrape_seasons(start_year, end_year):\n",
    "    \"\"\"\n",
    "    Scrapes data for each season from start_year to end_year.\n",
    "    \"\"\"\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        url = f\"https://www.basketball-reference.com/leagues/NBA_{year}_advanced.html#advanced\"\n",
    "        html_file = f\"NBA_{year}_advanced.html\"\n",
    "        csv_file = f\"NBA_{year}_advanced.csv\"\n",
    "        \n",
    "        download_webpage(url, html_file)\n",
    "        html_to_csv(html_file, csv_file)\n",
    "        \n",
    "scrape_seasons(2001, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NBA_2019_advanced.csv...\n",
      "Loading NBA_2018_advanced.csv...\n",
      "Loading NBA_2023_advanced.csv...\n",
      "Loading NBA_2024_advanced.csv...\n",
      "Loading NBA_2012_advanced.csv...\n",
      "Loading NBA_2015_advanced.csv...\n",
      "Loading NBA_2007_advanced.csv...\n",
      "Loading NBA_2022_advanced.csv...\n",
      "Loading NBA_2006_advanced.csv...\n",
      "Loading NBA_2001_advanced.csv...\n",
      "Loading NBA_2014_advanced.csv...\n",
      "Loading NBA_2013_advanced.csv...\n",
      "Loading NBA_2020_advanced.csv...\n",
      "Loading NBA_2003_advanced.csv...\n",
      "Loading NBA_2004_advanced.csv...\n",
      "Loading NBA_2011_advanced.csv...\n",
      "Loading NBA_2016_advanced.csv...\n",
      "Loading NBA_2021_advanced.csv...\n",
      "Loading NBA_2017_advanced.csv...\n",
      "Loading NBA_2010_advanced.csv...\n",
      "Loading NBA_2005_advanced.csv...\n",
      "Loading NBA_2002_advanced.csv...\n",
      "Loading NBA_2008_advanced.csv...\n",
      "Loading NBA_2009_advanced.csv...\n",
      "All CSV files merged successfully into /Users/nicholasrichards/Desktop/nba_advancedstats_csvs/nba_advancedstats_merged.csv\n"
     ]
    }
   ],
   "source": [
    "def merge_csv_files(directory, output_file):\n",
    "    \"\"\"\n",
    "    Merges all CSV files in the specified directory into a single CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "        \n",
    "        all_data = []\n",
    "\n",
    "        for csv_file in csv_files:\n",
    "            file_path = os.path.join(directory, csv_file)\n",
    "            print(f\"Loading {csv_file}...\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            season = csv_file.split('_')[1]\n",
    "            df['Season'] = season\n",
    "            all_data.append(df)\n",
    "\n",
    "        merged_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "        merged_data.to_csv(output_file, index=False)\n",
    "        print(f\"All CSV files merged successfully into {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error while merging CSV files: {e}\")\n",
    "\n",
    "directory = '/Users/nicholasrichards/Desktop/nba_advancedstats_csvs'\n",
    "output_file = '/Users/nicholasrichards/Desktop/nba_advancedstats_csvs/nba_advancedstats_merged.csv'\n",
    "\n",
    "\n",
    "merge_csv_files(directory, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed players with MP less than 1000. Remaining players: 6600\n",
      "Updated CSV saved as /Users/nicholasrichards/Desktop/nba_advancedstats_csvs/nba_advancedstats_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "def remove_players_with_low_mp(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Removes players with MP (minutes played) less than 1000 from the CSV file and saves the updated file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "\n",
    "        if 'MP' in df.columns:\n",
    "            df_filtered = df[df['MP'] >= 1000]\n",
    "            print(f\"Removed players with MP less than 1000. Remaining players: {len(df_filtered)}\")\n",
    "        else:\n",
    "            print(\"'MP' column not found in the file.\")\n",
    "            return\n",
    "        \n",
    "        df_filtered.to_csv(output_file, index=False)\n",
    "        print(f\"Updated CSV saved as {output_file}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error while removing players with low MP: {e}\")\n",
    "\n",
    "input_file = '/Users/nicholasrichards/Desktop/nba_advancedstats_csvs/nba_advancedstats_noawards.csv'\n",
    "output_file = '/Users/nicholasrichards/Desktop/nba_advancedstats_csvs/nba_advancedstats_cleaned.csv'\n",
    "\n",
    "remove_players_with_low_mp(input_file, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file sorted by season and saved as /Users/nicholasrichards/Desktop/nba_advancedstats_csvs/nba_advancedstats_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "def sort_csv_by_season(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Sorts the CSV file by the 'Season' column, from 2001 to 2024.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "\n",
    "        df['Season'] = pd.to_numeric(df['Season'], errors='coerce')\n",
    "\n",
    "        df_sorted = df.sort_values(by='Season', ascending=True)\n",
    "\n",
    "        df_sorted.to_csv(output_file, index=False)\n",
    "        print(f\"CSV file sorted by season and saved as {output_file}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error while sorting the CSV: {e}\")\n",
    "\n",
    "input_file = '/Users/nicholasrichards/Desktop/nba_advancedstats_csvs/nba_advancedstats_cleaned.csv'\n",
    "output_file = '/Users/nicholasrichards/Desktop/nba_advancedstats_csvs/nba_advancedstats_cleaned.csv'\n",
    "\n",
    "sort_csv_by_season(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data has been saved to /Users/nicholasrichards/Desktop/nba_combinedstats.csv\n",
      "Number of rows in the merged dataset: 6600\n"
     ]
    }
   ],
   "source": [
    "#This program merges the three CSVs just scraped and cleaned into one master CSV\n",
    "\n",
    "shooting_csv = \"/Users/nicholasrichards/Desktop/nba_shootingstats_csvs/nba_shootingstats_cleaned.csv\"\n",
    "totals_csv = \"/Users/nicholasrichards/Desktop/nba_totalstats_csvs/nba_totalstats_cleaned.csv\"\n",
    "advanced_csv = \"/Users/nicholasrichards/Desktop/nba_advancedstats_csvs/nba_advancedstats_cleaned.csv\"\n",
    "\n",
    "output_csv = \"/Users/nicholasrichards/Desktop/nba_combinedstats.csv\"\n",
    "\n",
    "shooting_df = pd.read_csv(shooting_csv)\n",
    "totals_df = pd.read_csv(totals_csv)\n",
    "advanced_df = pd.read_csv(advanced_csv)\n",
    "\n",
    "merge_columns = [\"Player\", \"Age\", \"Team\", \"Pos\", \"G\", \"GS\", \"MP\", \"Season\"]\n",
    "\n",
    "merged_df = shooting_df.merge(totals_df, on=merge_columns, how=\"inner\", suffixes=(\"_shooting\", \"_totals\"))\n",
    "merged_df = merged_df.merge(advanced_df, on=merge_columns, how=\"inner\")\n",
    "\n",
    "merged_df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Merged data has been saved to {output_csv}\")\n",
    "print(f\"Number of rows in the merged dataset: {len(merged_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to /Users/nicholasrichards/Desktop/machine_learning/nba_combinedstats_cleaned.csv\n",
      "Number of remaining columns: 55\n",
      "Number of rows: 6600\n"
     ]
    }
   ],
   "source": [
    "# Input and output file paths\n",
    "input_csv = \"/Users/nicholasrichards/Desktop/machine_learning/nba_combinedstats.csv\"\n",
    "output_csv = \"/Users/nicholasrichards/Desktop/machine_learning/nba_combinedstats_cleaned.csv\"\n",
    "\n",
    "columns_to_remove = [\n",
    "    \"2P.2\", \"3P.2\", \"#\", \"Att.\", \"Md.\", \"Rk_totals\", \"FG%_totals\", \n",
    "    \"eFG\", \"PF\", \"PTS\", \"Trp-Dbl\", \"Rk\", \"TS%\", \"USG%\", \n",
    "    \"OWS\", \"DWS\", \"WS\", \"WS/48\", \"OBPM\", \"DBPM\", \"BPM\", \"VORP\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    cleaned_df = df.drop(columns=columns_to_remove, errors=\"ignore\")\n",
    "    \n",
    "    cleaned_df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(f\"Cleaned dataset saved to {output_csv}\")\n",
    "    print(f\"Number of remaining columns: {len(cleaned_df.columns)}\")\n",
    "    print(f\"Number of rows: {len(cleaned_df)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing the dataset: {e}\")\n",
    "\n",
    "#Cleaning the master CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           C       0.85      0.83      0.84        47\n",
      "          PF       0.46      0.59      0.52        59\n",
      "          PG       0.70      0.60      0.65        58\n",
      "          SF       0.54      0.41      0.47        61\n",
      "          SG       0.46      0.51      0.48        61\n",
      "\n",
      "    accuracy                           0.58       286\n",
      "   macro avg       0.60      0.59      0.59       286\n",
      "weighted avg       0.59      0.58      0.58       286\n",
      "\n",
      "Accuracy: 0.58\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/Users/nicholasrichards/Desktop/machine_learning/nba_combinedstats_cleaned.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "features = [\n",
    "    \"FG%\", \"AvgDist\", \"2PA%\", \"0-3A%\", \"3-10A%\", \"10-16A%\", \"16-3PA%\", \n",
    "    \"3PA%\", \"2P%\", \"0-3%\", \"3-10%\", \"10-16%\", \"16-3P%\", \"3P%\", \n",
    "    \"DnkFGA%\", \"Cnr3PA%\", \"Cnr3P%\", \"FT%\", \"FTr\", \"ORB%\", \"AST%\", \n",
    "    \"STL%\", \"BLK%\", \"TOV%\"\n",
    "]\n",
    "\n",
    "missing_features = [col for col in features + [\"Pos\", \"Season\"] if col not in df.columns]\n",
    "if missing_features:\n",
    "    raise ValueError(f\"Missing columns in the dataset: {missing_features}\")\n",
    "\n",
    "train_data = df[df[\"Season\"] != 2024]\n",
    "test_data = df[df[\"Season\"] == 2024]\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[\"Pos\"]\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[\"Pos\"]\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "\n",
    "#This was the beginning of our random forest training but instead continued on Wendy's computer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset saved to: /Users/nicholasrichards/Desktop/machine_learning/nba_combinedstats_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "original_file = '/Users/nicholasrichards/Desktop/machine_learning/nba_combinedstats.csv'\n",
    "cleaned_file = '/Users/nicholasrichards/Desktop/machine_learning/nba_combinedstats_cleaned.csv'\n",
    "output_file = '/Users/nicholasrichards/Desktop/machine_learning/nba_combinedstats_cleaned.csv'\n",
    "\n",
    "original_data = pd.read_csv(original_file)\n",
    "cleaned_data = pd.read_csv(cleaned_file)\n",
    "\n",
    "merged_data = pd.merge(\n",
    "    cleaned_data,\n",
    "    original_data[['Player', 'Age', 'Team', 'Pos', 'PTS']],\n",
    "    on=['Player', 'Age', 'Team', 'Pos'],\n",
    "    how='left' \n",
    ")\n",
    "\n",
    "merged_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Merged dataset saved to: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qtm151-f24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
