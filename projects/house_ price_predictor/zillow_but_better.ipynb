{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Be Zillow, But Better?\n",
    "\n",
    "In class, we discussed the neural networks don't tend to outperform tree based models when the input data is tabular in structure.  While this is generally true, one situation with tabular data where a neural network can make developing a predictive model less cumbersome is when the data set contains **lots** of categorical variables.  In this problem, you'll be working with a novel housing data set and attempting to build a model that predicts the sale price for homes based on a variety of input features.\n",
    "\n",
    "The data set `house_train.csv` contains 943 instances of houses that sold in Sacremento, CA between 2007 and 2010.  The sale price for each home is contained in the column `SalePrice`. `house_valid.csv` contains 254 instances with the sale price included.  `house_test_noval.csv` is a test set with 254 more instances, but no sale price.  Each instance is associated with 63 features that relate to the size of the home, recency of the build, neighborhood, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MSSubClass MSZoning  LotArea Street LotShape LandContour Utilities  \\\n",
      "0          20       RL     9600   Pave      Reg         Lvl    AllPub   \n",
      "1          50       RL    14115   Pave      IR1         Lvl    AllPub   \n",
      "2          20       RL    10084   Pave      Reg         Lvl    AllPub   \n",
      "3          60       RL    10382   Pave      IR1         Lvl    AllPub   \n",
      "4          50       RM     6120   Pave      Reg         Lvl    AllPub   \n",
      "\n",
      "  LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle  \\\n",
      "0       FR2       Gtl      Veenker      Feedr       Norm     1Fam     1Story   \n",
      "1    Inside       Gtl      Mitchel       Norm       Norm     1Fam     1.5Fin   \n",
      "2    Inside       Gtl      Somerst       Norm       Norm     1Fam     1Story   \n",
      "3    Corner       Gtl       NWAmes       PosN       Norm     1Fam     2Story   \n",
      "4    Inside       Gtl      OldTown     Artery       Norm     1Fam     1.5Fin   \n",
      "\n",
      "   OverallQual  OverallCond  YearBuilt  YearRemodAdd RoofStyle RoofMatl  \\\n",
      "0            6            8       1976          1976     Gable  CompShg   \n",
      "1            5            5       1993          1995     Gable  CompShg   \n",
      "2            8            5       2004          2005     Gable  CompShg   \n",
      "3            7            6       1973          1973     Gable  CompShg   \n",
      "4            7            5       1931          1950     Gable  CompShg   \n",
      "\n",
      "  Exterior1st Exterior2nd MasVnrType  MasVnrArea ExterQual ExterCond  \\\n",
      "0     MetalSd     MetalSd        NaN         0.0        TA        TA   \n",
      "1     VinylSd     VinylSd        NaN         0.0        TA        TA   \n",
      "2     VinylSd     VinylSd      Stone       186.0        Gd        TA   \n",
      "3     HdBoard     HdBoard      Stone       240.0        TA        TA   \n",
      "4     BrkFace     Wd Shng        NaN         0.0        TA        TA   \n",
      "\n",
      "  Foundation  BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF Heating  \\\n",
      "0     CBlock         978           0        284         1262    GasA   \n",
      "1       Wood         732           0         64          796    GasA   \n",
      "2      PConc        1369           0        317         1686    GasA   \n",
      "3     CBlock         859          32        216         1107    GasA   \n",
      "4     BrkTil           0           0        952          952    GasA   \n",
      "\n",
      "  HeatingQC CentralAir Electrical  1stFlrSF  2ndFlrSF  LowQualFinSF  \\\n",
      "0        Ex          Y      SBrkr      1262         0             0   \n",
      "1        Ex          Y      SBrkr       796       566             0   \n",
      "2        Ex          Y      SBrkr      1694         0             0   \n",
      "3        Ex          Y      SBrkr      1107       983             0   \n",
      "4        Gd          Y      FuseF      1022       752             0   \n",
      "\n",
      "   GrLivArea  BsmtFullBath  BsmtHalfBath  FullBath  HalfBath  BedroomAbvGr  \\\n",
      "0       1262             0             1         2         0             3   \n",
      "1       1362             1             0         1         1             1   \n",
      "2       1694             1             0         2         0             3   \n",
      "3       2090             1             0         2         1             3   \n",
      "4       1774             0             0         2         0             2   \n",
      "\n",
      "   KitchenAbvGr KitchenQual  TotRmsAbvGrd Functional  Fireplaces  GarageCars  \\\n",
      "0             1          TA             6        Typ           1           2   \n",
      "1             1          TA             5        Typ           0           2   \n",
      "2             1          Gd             7        Typ           1           2   \n",
      "3             1          TA             7        Typ           2           2   \n",
      "4             2          TA             8       Min1           2           2   \n",
      "\n",
      "   GarageArea PavedDrive  WoodDeckSF  OpenPorchSF  EnclosedPorch  3SsnPorch  \\\n",
      "0         460          Y         298            0              0          0   \n",
      "1         480          Y          40           30              0        320   \n",
      "2         636          Y         255           57              0          0   \n",
      "3         484          Y         235          204            228          0   \n",
      "4         468          Y          90            0            205          0   \n",
      "\n",
      "   ScreenPorch  PoolArea  MiscVal  MoSold  YrSold SaleType SaleCondition  \\\n",
      "0            0         0        0       5    2007       WD        Normal   \n",
      "1            0         0      700      10    2009       WD        Normal   \n",
      "2            0         0        0       8    2007       WD        Normal   \n",
      "3            0         0      350      11    2009       WD        Normal   \n",
      "4            0         0        0       4    2008       WD       Abnorml   \n",
      "\n",
      "   SalePrice  \n",
      "0     181500  \n",
      "1     143000  \n",
      "2     307000  \n",
      "3     200000  \n",
      "4     129900  \n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "train_df = pd.read_csv('/Users/nicholasrichards/Desktop/machine_learning_2/ProblemSet2/house_train.csv')\n",
    "val_df = pd.read_csv('/Users/nicholasrichards/Desktop/machine_learning_2/ProblemSet2/house_valid.csv')\n",
    "test_df = pd.read_csv('/Users/nicholasrichards/Desktop/machine_learning_2/ProblemSet2/house_test_noval.csv')\n",
    "\n",
    "# Display the head of all columns\n",
    "pd.set_option('display.max_columns', None)  # Ensure all columns are displayed\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main challenge with this data set is that the vast majority of the features are **categorical** and must be encoded to use them in various regression style models.  Identifying the categorical features is a little challenging here since there are so many.  Here, any variables about Area, Square Footage (SF), or Porch (annoyingly not containing SF or Area) are continuous measurements.  All other columns should be treated as categorical.\n",
    "\n",
    "**Note**: The training data was constructed as a subset of a larger data set in a way that ensures that each category for each categorical variable is included in the training data set at least once.  This means that there won't be any previously unseen categories in the validation and test data!  Please use this exact training data set when training your models to avoid running into prediction problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea']\n",
      "There are 17 continuous variables.\n",
      "['MSSubClass', 'MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'GarageCars', 'PavedDrive', 'MiscVal', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition']\n",
      "There are 46 categorical variables.\n"
     ]
    }
   ],
   "source": [
    "# Create lists of continuous and categorical column names\n",
    "continuous_x = [col for col in train_df.columns if any(substring in col.lower() for substring in [\"area\", \"sf\", \"porch\", \"saleprice\"])]\n",
    "\n",
    "cat_x = [col for col in train_df.columns if col not in continuous_x]\n",
    "\n",
    "continuous_x.remove('SalePrice')\n",
    "\n",
    "print(continuous_x)\n",
    "print(f\"There are {len(continuous_x)} continuous variables.\")\n",
    "\n",
    "print(cat_x)\n",
    "print(f\"There are {len(cat_x)} categorical variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common method of dealing with categorical variables is to one-hot encode them.  However, this will lead to a *very large* feature set relative to the number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSSubClass': 15, 'MSZoning': 5, 'Street': 2, 'LotShape': 4, 'LandContour': 4, 'Utilities': 2, 'LotConfig': 5, 'LandSlope': 3, 'Neighborhood': 25, 'Condition1': 9, 'Condition2': 8, 'BldgType': 5, 'HouseStyle': 8, 'OverallQual': 10, 'OverallCond': 9, 'YearBuilt': 112, 'YearRemodAdd': 61, 'RoofStyle': 6, 'RoofMatl': 8, 'Exterior1st': 15, 'Exterior2nd': 16, 'MasVnrType': 3, 'ExterQual': 4, 'ExterCond': 5, 'Foundation': 6, 'Heating': 6, 'HeatingQC': 5, 'CentralAir': 2, 'Electrical': 5, 'BsmtFullBath': 4, 'BsmtHalfBath': 3, 'FullBath': 4, 'HalfBath': 3, 'BedroomAbvGr': 8, 'KitchenAbvGr': 4, 'KitchenQual': 4, 'TotRmsAbvGrd': 12, 'Functional': 7, 'Fireplaces': 4, 'GarageCars': 5, 'PavedDrive': 3, 'MiscVal': 21, 'MoSold': 12, 'YrSold': 5, 'SaleType': 9, 'SaleCondition': 6}\n",
      "Total number of categories: 482\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of distinct values for each variable in cat_x\n",
    "distinct_values = {col: train_df[col].nunique() for col in cat_x}\n",
    "print(distinct_values)\n",
    "\n",
    "# Calculate the total number of dummies needed for a regression model\n",
    "total_dummies = sum(train_df[col].nunique() for col in cat_x)\n",
    "print(f\"Total number of categories: {total_dummies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, house prices follow a power distribution and a standard loss function like MSE will overrespond to big misses on high priced homes that have more to do with neighborhood effects than house features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a histogram of y_train\n",
    "plt.hist(train_df['SalePrice'], bins=30, edgecolor='black')\n",
    "plt.xlabel('SalePrice')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sale Prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is possible to change the MSE loss function to a more *robust* one (e.g. Huber loss) or deal with large categorical feature sets (CatBoost) in the tree framework, neural networks are much more suited for making these changes through their differentiable backpropogation framework.  Using PyTorch, changing the loss function is as simple as changing the loss function in your PyTorch module!  Additionally, categorical feature spaces can be easily dealt with using an **embeddings** framework.\n",
    "\n",
    "For this question, you'll build models that attempt to best predict the house price for the houses included in the test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: Standardization and Encoding\n",
    "\n",
    "For the purposes of machine learning, features are typically assumed to be either continuous or categorical.  It is possible to deal with ordered categorical variables in a different way, but most of the time, they are treated as unordered categorical variables.\n",
    "\n",
    "As we saw in class, one way to prevent exploding and vanishing gradients is to ensure that any continuous variables are demeaned and scaled to have unit variance.  Since we aren't concerned with interpreting any coefficients, removing this scale is completely okay.\n",
    "\n",
    "When we're working with training data to develop models that will be used to create predictions for new data, we need to be careful how we center and scale columns.  Specifically, we need to be sure that we're storing the mean and variance of each column in the training data so that the same transformation can be done to any feature vectors that will be used to create predictions later.\n",
    "\n",
    "Additionally, we're going to scale the outcome variable to better facilitate weights close to zero and avoid gradient problems.\n",
    "\n",
    "Similarly, categorical variables need to be encoded using integer labels since there isn't really a way to enter character strings in a matrix.  How we choose to do this encoding needs to be based on the training data and the logic for how these category values are mapped to integers also needs to be cosnsistently applied to create a prediction pipeline.\n",
    "\n",
    "Fortunately, `sklearn` provides a streamlined way to do this!  Given some indication of which variables are continuous and which variables are categorical, we can develop a data pre-processing pipeline that can be consistently applied to any new feature vector that has the same organization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MSSubClass  MSZoning   LotArea  Street  LotShape  LandContour  Utilities  \\\n",
      "0           0         3 -0.104724       1         3            3          0   \n",
      "1           4         3  0.358146       1         0            3          0   \n",
      "2           0         3 -0.055105       1         3            3          0   \n",
      "3           5         3 -0.024555       1         0            3          0   \n",
      "4           4         4 -0.461488       1         3            3          0   \n",
      "\n",
      "   LotConfig  LandSlope  Neighborhood  Condition1  Condition2  BldgType  \\\n",
      "0          2          0            24           1           2         0   \n",
      "1          4          0            11           2           2         0   \n",
      "2          4          0            21           2           2         0   \n",
      "3          0          0            14           4           2         0   \n",
      "4          4          0            17           0           2         0   \n",
      "\n",
      "   HouseStyle  OverallQual  OverallCond  YearBuilt  YearRemodAdd  RoofStyle  \\\n",
      "0           2            5            7         77            26          1   \n",
      "1           0            4            4         94            45          1   \n",
      "2           2            7            4        105            55          1   \n",
      "3           5            6            5         74            23          1   \n",
      "4           0            6            4         35             0          1   \n",
      "\n",
      "   RoofMatl  Exterior1st  Exterior2nd  MasVnrType  MasVnrArea  ExterQual  \\\n",
      "0         1            8            8           3   -0.538598          3   \n",
      "1         1           12           13           3   -0.538598          3   \n",
      "2         1           12           13           2    0.494054          2   \n",
      "3         1            6            6           2    0.793856          3   \n",
      "4         1            3           15           3   -0.538598          3   \n",
      "\n",
      "   ExterCond  Foundation  BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF  \\\n",
      "0          4           1    1.126028   -0.290871  -0.592625     0.499132   \n",
      "1          4           5    0.604268   -0.290871  -1.092552    -0.544840   \n",
      "2          4           2    1.955329   -0.290871  -0.517636     1.449012   \n",
      "3          4           1    0.873632   -0.094127  -0.747148     0.151888   \n",
      "4          4           0   -0.948285   -0.290871   0.925335    -0.195356   \n",
      "\n",
      "   Heating  HeatingQC  CentralAir  Electrical  1stFlrSF  2ndFlrSF  \\\n",
      "0        1          0           1           4  0.271883 -0.788990   \n",
      "1        1          0           1           4 -0.919597  0.534706   \n",
      "2        1          0           1           4  1.376430 -0.788990   \n",
      "3        1          0           1           4 -0.124425  1.509937   \n",
      "4        1          2           1           1 -0.341755  0.969701   \n",
      "\n",
      "   LowQualFinSF  GrLivArea  BsmtFullBath  BsmtHalfBath  FullBath  HalfBath  \\\n",
      "0     -0.126859  -0.454099             0             1         2         0   \n",
      "1     -0.126859  -0.262810             1             0         1         1   \n",
      "2     -0.126859   0.372273             1             0         2         0   \n",
      "3     -0.126859   1.129781             1             0         2         1   \n",
      "4     -0.126859   0.525305             0             0         2         0   \n",
      "\n",
      "   BedroomAbvGr  KitchenAbvGr  KitchenQual  TotRmsAbvGrd  Functional  \\\n",
      "0             3             1            3             4           6   \n",
      "1             1             1            3             3           6   \n",
      "2             3             1            2             5           6   \n",
      "3             3             1            3             5           6   \n",
      "4             2             2            3             6           2   \n",
      "\n",
      "   Fireplaces  GarageCars  GarageArea  PavedDrive  WoodDeckSF  OpenPorchSF  \\\n",
      "0           1           2   -0.034451           2    1.639831    -0.668511   \n",
      "1           0           2    0.059647           2   -0.437929    -0.227801   \n",
      "2           1           2    0.793615           2    1.293537     0.168838   \n",
      "3           2           2    0.078467           2    1.132471     2.328316   \n",
      "4           2           2    0.003188           2   -0.035262    -0.668511   \n",
      "\n",
      "   EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal  MoSold  YrSold  \\\n",
      "0      -0.371011  -0.124694    -0.265635 -0.072618        0       4       1   \n",
      "1      -0.371011  10.158910    -0.265635 -0.072618       10       9       3   \n",
      "2      -0.371011  -0.124694    -0.265635 -0.072618        0       7       1   \n",
      "3       3.244982  -0.124694    -0.265635 -0.072618        2      10       3   \n",
      "4       2.880211  -0.124694    -0.265635 -0.072618        0       3       2   \n",
      "\n",
      "   SaleType  SaleCondition  SalePrice  \n",
      "0         8              4   0.048547  \n",
      "1         8              4  -0.440558  \n",
      "2         8              4   1.642901  \n",
      "3         8              4   0.283571  \n",
      "4         8              0  -0.606980  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Initialize the scaler and encoders\n",
    "scaler = StandardScaler()\n",
    "price_scaler = StandardScaler()\n",
    "encoders = {col: LabelEncoder() for col in cat_x}\n",
    "\n",
    "# Fit the scaler on the continuous features of the training data\n",
    "train_df[continuous_x] = scaler.fit_transform(train_df[continuous_x])\n",
    "\n",
    "# Scale 'SalePrice' separately if it is in the data\n",
    "if 'SalePrice' in train_df.columns:\n",
    "    train_df['SalePrice'] = price_scaler.fit_transform(train_df[['SalePrice']])\n",
    "\n",
    "# Fit the encoders on the categorical features of the training data\n",
    "for col in cat_x:\n",
    "    train_df[col] = encoders[col].fit_transform(train_df[col])\n",
    "\n",
    "# Apply the same transformations to the validation and test data\n",
    "val_df[continuous_x] = scaler.transform(val_df[continuous_x])\n",
    "test_df[continuous_x] = scaler.transform(test_df[continuous_x])\n",
    "\n",
    "if 'SalePrice' in val_df.columns:\n",
    "    val_df['SalePrice'] = price_scaler.transform(val_df[['SalePrice']])\n",
    "if 'SalePrice' in test_df.columns:\n",
    "    test_df['SalePrice'] = price_scaler.transform(test_df[['SalePrice']])\n",
    "\n",
    "for col in cat_x:\n",
    "    val_df[col] = encoders[col].transform(val_df[col])\n",
    "    test_df[col] = encoders[col].transform(test_df[col])\n",
    "\n",
    "# Display the transformed dataframes\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data sets are standardized and encoded, we have to choose some way to numerically represent the fact that integer encoding labels are arbitrary representations of a categorical variable.\n",
    "\n",
    "When dealing with categorical variables, the conventional approach is to use **one‑hot encoding**. In one‑hot encoding, each category is represented as a vector whose length is equal to the total number of categories, with a single element set to 1 (indicating the presence of that category) and all other elements set to 0. Mathematically, if a categorical variable can take on one of $C$ values, a particular category $i$ is represented by the vector: \n",
    "\n",
    "$$\\mathbf{e}_i = \\left[0, \\ldots, 0, \\underset{\\text{position } i}{1}, 0, \\ldots, 0\\right]^T$$ \n",
    "\n",
    "which is an element of $\\mathbb{R}^C$. When you have many categorical variables, concatenating these one‑hot vectors results in a very high‑dimensional and sparse feature matrix. Such matrices are not only memory‑inefficient but also computationally expensive to work with.\n",
    "\n",
    "**Embeddings** provide a way to represent categorical variables in a much lower‑dimensional continuous space. An embedding layer maintains a matrix: \n",
    "\n",
    "$$E \\in \\mathbb{R}^{C \\times d}$$ \n",
    "\n",
    "where $C$ is the number of categories and $d$ is the embedding dimension (with $d \\ll C$ in many cases). Each row of $E$ corresponds to a dense, learnable vector for a category. Conceptually, an embedding layer acts as a lookup table: given a category $i$, the layer retrieves the $i$‑th row $\\mathbf{w}_i \\in \\mathbb{R}^d$.\n",
    "\n",
    "Interestingly, if we set the embedding dimension $d$ to 1, each category is represented by a single scalar rather than a high‑dimensional vector. In this case, the embedding matrix becomes \n",
    "\n",
    "$$E \\in \\mathbb{R}^{C \\times 1}$$ \n",
    "\n",
    "and the lookup for category $i$ simply returns the scalar $w_i$. In many linear models, one‑hot encoding leads to a separate coefficient for each category. Using a 1‑dimensional embedding replicates this idea: the scalar $w_i$ serves as the learned effect or offset for category $i$. More importantly, this approach achieves the same outcome as one‑hot encoding without ever constructing an explicit $C$‑dimensional vector for each category.\n",
    "\n",
    "To put it another way, if you were to one‑hot encode a category and then apply a linear transformation with weight matrix $E^T$, you would compute: \n",
    "\n",
    "$$\\hat{w}_i = E^T \\mathbf{e}_i = w_i$$ \n",
    "\n",
    "which is exactly the scalar produced by the embedding lookup when $d=1$. Thus, a 1‑dimensional embedding acts as a compact representation equivalent to having separate parameters for each category—as is typical in one‑hot encoding—while avoiding the overhead of managing large, sparse vectors.\n",
    "\n",
    "Multidimensional embeddings ($d > 1$) are useful because they allow us to capture more complex relationships and interactions between categories. By representing each category as a dense vector in a continuous space, we can learn meaningful patterns and similarities between categories that would be difficult to capture with one-hot encoding. This can lead to better model performance, especially when dealing with high-cardinality categorical variables. Additionally, embeddings reduce the dimensionality of the feature space, making the model more efficient and less prone to overfitting.\n",
    "\n",
    "For example, consider a recommendation system for an e-commerce platform. The platform has millions of users and products, each with various categorical attributes such as user demographics, product categories, and user interactions. Using one-hot encoding for these categorical variables would result in a very high-dimensional and sparse feature matrix, making the model computationally expensive and memory-inefficient. By using multidimensional embeddings, we can represent each user and product as dense vectors in a continuous space, capturing complex relationships and interactions between them. This allows the recommendation system to learn meaningful patterns and similarities, leading to more accurate and personalized recommendations for users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement embeddings in PyTorch, we'll need to leverage a custom data loader that differentiates between categorical and continuous variables.  We can do this by first constructing a custom data set class.  Then, we can pass our custom data set to a PyTorch dataloader.\n",
    "\n",
    "To ensure that the DataLoader format will work, PyTorch requires that all data be in a form that meets the following conditions:\n",
    "\n",
    "- All partitions within the data can be separately accessed\n",
    "\n",
    "- All data is stored in the PyTorch **tensor** format with an appropriate numeric type\n",
    "\n",
    "- All data can be accessed using a `__getitem__` method and its length can be accessed with a `__len__` method.\n",
    "\n",
    "Below is one way to set this up to have three different data types: continuous features, categorical features, and the outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: target column 'SalePrice' not found in dataframe. Setting target to None.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, continuous_cols, categorical_cols, target_col):\n",
    "        self.continuous = torch.tensor(df[continuous_cols].values, dtype=torch.float32)\n",
    "        self.categorical = torch.tensor(df[categorical_cols].values, dtype=torch.long)\n",
    "        #self.target = torch.tensor(df[target_col].values, dtype=torch.float32)\n",
    "        if target_col is not None:\n",
    "            try:\n",
    "                self.target = torch.tensor(df[target_col].values, dtype=torch.float32)\n",
    "            except KeyError:\n",
    "                print(f\"Warning: target column '{target_col}' not found in dataframe. Setting target to None.\")\n",
    "                self.target = None\n",
    "        else:\n",
    "            self.target = None        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.continuous[idx], self.categorical[idx], self.target[idx]\n",
    "    \n",
    "# Create datasets for train, validation, and test\n",
    "train_dataset = CustomDataset(train_df, continuous_x, cat_x, 'SalePrice')\n",
    "valid_dataset = CustomDataset(val_df, continuous_x, cat_x, 'SalePrice')\n",
    "test_dataset = CustomDataset(test_df, continuous_x, cat_x, 'SalePrice')\n",
    "\n",
    "# Create DataLoaders for train, validation, and test\n",
    "train_loader = DataLoader(train_dataset, batch_size= 200, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_df.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data loader will yield three different tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Features: tensor([[ 0.1639,  1.1547,  2.0974, -0.2909,  0.0573,  2.1659,  2.2100, -0.7890,\n",
      "         -0.1269,  0.9959,  2.0357,  0.4963,  0.1248, -0.3710, -0.1247, -0.2656,\n",
      "         -0.0726]])\n",
      "Categorical Features: tensor([[  0,   3,   1,   3,   3,   0,   4,   0,  16,   2,   2,   0,   2,   9,\n",
      "           4, 110,  59,   3,   1,   5,   5,   1,   0,   4,   1,   1,   0,   1,\n",
      "           4,   1,   0,   2,   1,   3,   1,   0,   7,   6,   1,   3,   2,   0,\n",
      "           8,   3,   6,   5]])\n",
      "Targets: tensor([2.8607])\n"
     ]
    }
   ],
   "source": [
    "for batch in valid_loader:\n",
    "    continuous, cat, targets = batch\n",
    "    print(\"Continuous Features:\", continuous)\n",
    "    print(\"Categorical Features:\", cat)\n",
    "    print(\"Targets:\", targets) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for train, validation, and test\n",
    "train_loader = DataLoader(train_dataset, batch_size= train_df.shape[0], shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=val_df.shape[0], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_df.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show how the embeddings layer works, I'll implement a simple neural network with Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "\n",
    "class DeepRegressionWithEmbeddings(pl.LightningModule):\n",
    "    def __init__(self, num_categories, embedding_dims, num_continuous_features, lr=1e-3):\n",
    "        super().__init__()\n",
    "        # Save all the hyperparameters automatically so they can be inherited when calling checkpoints\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Handle embedding dimensions input\n",
    "        if isinstance(embedding_dims, int):\n",
    "            embedding_dims = [embedding_dims] * len(num_categories)\n",
    "\n",
    "        # Add a check\n",
    "        assert len(num_categories) == len(embedding_dims), \"Number of categories and embedding dimensions must match\"\n",
    "\n",
    "        # Create embedding layers\n",
    "        # Group together a list of column embeddings\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            # For each column in the categorical variables map each category\n",
    "            # To a vector of length embedding_dims\n",
    "            nn.Embedding(num_categories[i], embedding_dims[i]) for i in range(len(num_categories))\n",
    "        ])\n",
    "\n",
    "        # Compute the total number of features after embedding\n",
    "        total_embedding_dim = sum(embedding_dims)\n",
    "\n",
    "        #Define a single hidden layer that comes after the embeddings\n",
    "        #Concatenate the continuous embeddings for the categorical variables\n",
    "        #With the continuous values\n",
    "        self.hidden1 = nn.Linear(total_embedding_dim + num_continuous_features,128)\n",
    "\n",
    "        #Activation function\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        #BatchNorm Layer\n",
    "        self.batchnorm1 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define linear layer\n",
    "        self.regressionlayer = nn.Linear(128, 1)\n",
    "\n",
    "        # Define loss function\n",
    "        self.criterion = nn.L1Loss() \n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, continuous, categorical):\n",
    "        # Pass categorical features through embeddings\n",
    "        embedded = [emb(categorical[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embedded, dim=1)\n",
    "\n",
    "        # Concatenate continuous features with embedded categorical features\n",
    "        features = torch.cat([continuous, embedded], dim=1)\n",
    "\n",
    "        x = self.hidden1(features)\n",
    "        x = self.act(x)\n",
    "        x = self.batchnorm1(x)\n",
    "\n",
    "        # Pass through linear layer\n",
    "        output = self.regressionlayer(x)\n",
    "        return output.squeeze(1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        continuous, categorical, target = batch\n",
    "        predictions = self(continuous, categorical)\n",
    "        loss = self.criterion(predictions, target)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        continuous, categorical, target = batch\n",
    "        predictions = self(continuous, categorical)\n",
    "        loss = self.criterion(predictions, target)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        continuous, categorical, _ = batch\n",
    "        predictions = self(continuous, categorical)\n",
    "        return predictions\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is set up to take a list with the number of categories for each categorical column and a list with the number of embedding dimensions for each categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 5, 2, 4, 4, 2, 5, 3, 25, 9, 8, 5, 8, 10, 9, 112, 61, 6, 8, 15, 16, 4, 4, 5, 6, 6, 5, 2, 5, 4, 3, 4, 3, 8, 4, 4, 12, 7, 4, 5, 3, 21, 12, 5, 9, 6]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "num_cats = [train_df[col].nunique() for col in cat_x]\n",
    "print(num_cats)\n",
    "\n",
    "embedding_dims = [1]*len(num_cats)\n",
    "print(embedding_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the lightning trainer to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory PS2Q3/Model1/ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/nicholasrichards/Desktop/machine_learning_2/PS2Q3/Model1 exists and is not empty.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name            </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type        </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ embeddings      │ ModuleList  │    483 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ hidden1         │ Linear      │  8.2 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ act             │ ReLU        │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ batchnorm1      │ BatchNorm1d │    256 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ regressionlayer │ Linear      │    129 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ criterion       │ L1Loss      │      0 │ train │\n",
       "└───┴─────────────────┴─────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName           \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType       \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ embeddings      │ ModuleList  │    483 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ hidden1         │ Linear      │  8.2 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ act             │ ReLU        │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ batchnorm1      │ BatchNorm1d │    256 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ regressionlayer │ Linear      │    129 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ criterion       │ L1Loss      │      0 │ train │\n",
       "└───┴─────────────────┴─────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 9.1 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 9.1 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 52                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 9.1 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 9.1 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 52                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/trainer/co\n",
       "nnectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider\n",
       "increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/trainer/co\n",
       "nnectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider\n",
       "increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/trainer/co\n",
       "nnectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. \n",
       "Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve \n",
       "performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/trainer/co\n",
       "nnectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. \n",
       "Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve \n",
       "performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/loops/fit_\n",
       "loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50).\n",
       "Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/loops/fit_\n",
       "loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50).\n",
       "Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, RichProgressBar\n",
    "\n",
    "folder_name = \"Model1\"\n",
    "\n",
    "# CSVLogger: All CSV logs will be saved in \"PS2Q2/{folder_name}\".\n",
    "logger = CSVLogger(\"PS2Q3\", name=folder_name, flush_logs_every_n_steps=1, version=\"\")\n",
    "\n",
    "# ModelCheckpoint: Save the best model (lowest validation loss) in the same folder.\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"PS2Q3/{folder_name}\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "# RichProgressBar: For a nicer progress bar.\n",
    "rich_bar = RichProgressBar()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Model Instantiation and Training\n",
    "# Number of input features\n",
    "# List of Hidden Units\n",
    "# Learning Rate\n",
    "# ------------------------------------------------------------\n",
    "model = DeepRegressionWithEmbeddings(num_cats, embedding_dims, len(continuous_x), .1)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Trainer Setup\n",
    "# Max epochs\n",
    "# Auto find the best hardware for running (GPU if available)\n",
    "# Set the logger to be our CSV logger\n",
    "# Create two callbacks: checkpoint creation, and the rich progress bar\n",
    "# ------------------------------------------------------------\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator='auto',\n",
    "    devices='auto',\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, rich_bar]\n",
    ")\n",
    "\n",
    "# Start training. Here, we pass our DataLoaders externally.\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, get the predictions for the validation data set at the lowest validation error model state.  Transform both the predicted and true prices back to the original scale and plot the predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the best checkpoint model\n",
    "best_model = DeepRegressionWithEmbeddings.load_from_checkpoint(f\"PS2Q3/{folder_name}/best-checkpoint.ckpt\")\n",
    "\n",
    "# Make predictions on the validation data\n",
    "predictions = trainer.predict(best_model, dataloaders=valid_loader)\n",
    "predictions = torch.cat(predictions).detach().numpy()\n",
    "\n",
    "# Transform the predictions and true prices back to the original price scale\n",
    "predictions_original_scale = price_scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "true_prices_original_scale = price_scaler.inverse_transform(val_df['SalePrice'].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot the true prices against the predicted prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(true_prices_original_scale, predictions_original_scale, alpha=0.6)\n",
    "plt.plot([true_prices_original_scale.min(), true_prices_original_scale.max()],\n",
    "         [true_prices_original_scale.min(), true_prices_original_scale.max()], 'r--')\n",
    "plt.xlabel('True Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "plt.title('True Prices vs. Predicted Prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazingly, we never had to represent 450 features!  The embeddings layer did all of the work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (and only)\n",
    "\n",
    "Using everything you've learned about regression, embeddings, neural networks, generalization, etc., train a differentiable model using PyTorch that will do a good job of **robustly** predicting the price of the homes that are included in the held out test data set.\n",
    "\n",
    "My advice is to start with basic linear regression, then play around with hidden layers.  Finally, I would explore heteroskedastic regression models as well as stochastic volatility models.  You can see an example of this in 13.2.4.4 of PML1.\n",
    "\n",
    "I will judge your predictions using an automated checker that can be found at XXX.  Submit your predictions in the requested form.  You may submit as many models as you would like to be judged.\n",
    "\n",
    "Your predictions will be judged using a specific loss function that is not MSE.  The loss function will be symmetric around zero and will penalize rare big misses less harshly than a lot of small misses.  As such, you'll want to play around with different loss functions included in `torch.nn` to find a model that seems to perform well for the validation data set.\n",
    "\n",
    "When creating predictions for the test data set, be aware that there is an additional ID column that should be used to identify your predictions for the checker.  Be sure not to accidentally include this column when you make your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous columns: ['LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea']\n",
      "Number of continuous columns: 17\n",
      "Categorical columns: ['MSSubClass', 'MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'GarageCars', 'PavedDrive', 'MiscVal', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition']\n",
      "Number of categorical columns: 46\n",
      "Distinct values per categorical variable:\n",
      "{'MSSubClass': 15, 'MSZoning': 5, 'Street': 2, 'LotShape': 4, 'LandContour': 4, 'Utilities': 2, 'LotConfig': 5, 'LandSlope': 3, 'Neighborhood': 25, 'Condition1': 9, 'Condition2': 8, 'BldgType': 5, 'HouseStyle': 8, 'OverallQual': 10, 'OverallCond': 9, 'YearBuilt': 112, 'YearRemodAdd': 61, 'RoofStyle': 6, 'RoofMatl': 8, 'Exterior1st': 15, 'Exterior2nd': 16, 'MasVnrType': 3, 'ExterQual': 4, 'ExterCond': 5, 'Foundation': 6, 'Heating': 6, 'HeatingQC': 5, 'CentralAir': 2, 'Electrical': 5, 'BsmtFullBath': 4, 'BsmtHalfBath': 3, 'FullBath': 4, 'HalfBath': 3, 'BedroomAbvGr': 8, 'KitchenAbvGr': 4, 'KitchenQual': 4, 'TotRmsAbvGrd': 12, 'Functional': 7, 'Fireplaces': 4, 'GarageCars': 5, 'PavedDrive': 3, 'MiscVal': 21, 'MoSold': 12, 'YrSold': 5, 'SaleType': 9, 'SaleCondition': 6}\n",
      "Processed training data:\n",
      "   MSSubClass  MSZoning   LotArea  Street  LotShape  LandContour  Utilities  \\\n",
      "0           0         3 -0.104724       1         3            3          0   \n",
      "1           4         3  0.358146       1         0            3          0   \n",
      "2           0         3 -0.055105       1         3            3          0   \n",
      "3           5         3 -0.024555       1         0            3          0   \n",
      "4           4         4 -0.461488       1         3            3          0   \n",
      "\n",
      "   LotConfig  LandSlope  Neighborhood  Condition1  Condition2  BldgType  \\\n",
      "0          2          0            24           1           2         0   \n",
      "1          4          0            11           2           2         0   \n",
      "2          4          0            21           2           2         0   \n",
      "3          0          0            14           4           2         0   \n",
      "4          4          0            17           0           2         0   \n",
      "\n",
      "   HouseStyle  OverallQual  OverallCond  YearBuilt  YearRemodAdd  RoofStyle  \\\n",
      "0           2            5            7         77            26          1   \n",
      "1           0            4            4         94            45          1   \n",
      "2           2            7            4        105            55          1   \n",
      "3           5            6            5         74            23          1   \n",
      "4           0            6            4         35             0          1   \n",
      "\n",
      "   RoofMatl  Exterior1st  Exterior2nd  MasVnrType  MasVnrArea  ExterQual  \\\n",
      "0         1            8            8           3   -0.538598          3   \n",
      "1         1           12           13           3   -0.538598          3   \n",
      "2         1           12           13           2    0.494054          2   \n",
      "3         1            6            6           2    0.793856          3   \n",
      "4         1            3           15           3   -0.538598          3   \n",
      "\n",
      "   ExterCond  Foundation  BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF  \\\n",
      "0          4           1    1.126028   -0.290871  -0.592625     0.499132   \n",
      "1          4           5    0.604268   -0.290871  -1.092552    -0.544840   \n",
      "2          4           2    1.955329   -0.290871  -0.517636     1.449012   \n",
      "3          4           1    0.873632   -0.094127  -0.747148     0.151888   \n",
      "4          4           0   -0.948285   -0.290871   0.925335    -0.195356   \n",
      "\n",
      "   Heating  HeatingQC  CentralAir  Electrical  1stFlrSF  2ndFlrSF  \\\n",
      "0        1          0           1           4  0.271883 -0.788990   \n",
      "1        1          0           1           4 -0.919597  0.534706   \n",
      "2        1          0           1           4  1.376430 -0.788990   \n",
      "3        1          0           1           4 -0.124425  1.509937   \n",
      "4        1          2           1           1 -0.341755  0.969701   \n",
      "\n",
      "   LowQualFinSF  GrLivArea  BsmtFullBath  BsmtHalfBath  FullBath  HalfBath  \\\n",
      "0     -0.126859  -0.454099             0             1         2         0   \n",
      "1     -0.126859  -0.262810             1             0         1         1   \n",
      "2     -0.126859   0.372273             1             0         2         0   \n",
      "3     -0.126859   1.129781             1             0         2         1   \n",
      "4     -0.126859   0.525305             0             0         2         0   \n",
      "\n",
      "   BedroomAbvGr  KitchenAbvGr  KitchenQual  TotRmsAbvGrd  Functional  \\\n",
      "0             3             1            3             4           6   \n",
      "1             1             1            3             3           6   \n",
      "2             3             1            2             5           6   \n",
      "3             3             1            3             5           6   \n",
      "4             2             2            3             6           2   \n",
      "\n",
      "   Fireplaces  GarageCars  GarageArea  PavedDrive  WoodDeckSF  OpenPorchSF  \\\n",
      "0           1           2   -0.034451           2    1.639831    -0.668511   \n",
      "1           0           2    0.059647           2   -0.437929    -0.227801   \n",
      "2           1           2    0.793615           2    1.293537     0.168838   \n",
      "3           2           2    0.078467           2    1.132471     2.328316   \n",
      "4           2           2    0.003188           2   -0.035262    -0.668511   \n",
      "\n",
      "   EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal  MoSold  YrSold  \\\n",
      "0      -0.371011  -0.124694    -0.265635 -0.072618        0       4       1   \n",
      "1      -0.371011  10.158910    -0.265635 -0.072618       10       9       3   \n",
      "2      -0.371011  -0.124694    -0.265635 -0.072618        0       7       1   \n",
      "3       3.244982  -0.124694    -0.265635 -0.072618        2      10       3   \n",
      "4       2.880211  -0.124694    -0.265635 -0.072618        0       3       2   \n",
      "\n",
      "   SaleType  SaleCondition  SalePrice  \n",
      "0         8              4   0.048547  \n",
      "1         8              4  -0.440558  \n",
      "2         8              4   1.642901  \n",
      "3         8              4   0.283571  \n",
      "4         8              0  -0.606980  \n",
      "Processed validation data:\n",
      "   MSSubClass  MSZoning   LotArea  Street  LotShape  LandContour  Utilities  \\\n",
      "0           0         3  0.163874       1         3            3          0   \n",
      "1          12         4 -0.931739       1         3            3          0   \n",
      "2          14         3 -0.328214       1         3            3          0   \n",
      "3           5         3  3.021470       1         0            0          0   \n",
      "4           5         3  0.133529       1         0            3          0   \n",
      "\n",
      "   LotConfig  LandSlope  Neighborhood  Condition1  Condition2  BldgType  \\\n",
      "0          4          0            16           2           2         0   \n",
      "1          4          0            10           2           2         3   \n",
      "2          0          0             3           0           0         1   \n",
      "3          4          0             7           4           4         0   \n",
      "4          4          0            16           2           2         0   \n",
      "\n",
      "   HouseStyle  OverallQual  OverallCond  YearBuilt  YearRemodAdd  RoofStyle  \\\n",
      "0           2            9            4        110            59          3   \n",
      "1           5            3            5         71            58          1   \n",
      "2           1            4            5         42             0          1   \n",
      "3           5            9            4        108            58          3   \n",
      "4           5            8            4        106            56          3   \n",
      "\n",
      "   RoofMatl  Exterior1st  Exterior2nd  MasVnrType  MasVnrArea  ExterQual  \\\n",
      "0         1            5            5           1    1.154729          0   \n",
      "1         1            5            5           3   -0.538598          3   \n",
      "2         1            8            8           3   -0.538598          3   \n",
      "3         1            5            5           2    3.691943          0   \n",
      "4         1           14           15           2    1.049243          0   \n",
      "\n",
      "   ExterCond  Foundation  BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF  \\\n",
      "0          4           1    2.097434   -0.290871   0.057280     2.165902   \n",
      "1          4           1   -0.948285   -0.290871   0.002742    -1.104911   \n",
      "2          4           0    0.856664   -0.290871  -0.919850    -0.107985   \n",
      "3          4           2    3.845117   -0.290871   0.757177     4.701903   \n",
      "4          4           2    1.168447   -0.290871  -0.835771     0.304227   \n",
      "\n",
      "   Heating  HeatingQC  CentralAir  Electrical  1stFlrSF  2ndFlrSF  \\\n",
      "0        1          0           1           4  2.209955 -0.788990   \n",
      "1        1          4           1           4 -0.914483  0.487932   \n",
      "2        1          0           1           4 -0.201130 -0.788990   \n",
      "3        1          0           1           4  5.068483  2.807907   \n",
      "4        1          0           1           4  0.067337  1.881788   \n",
      "\n",
      "   LowQualFinSF  GrLivArea  BsmtFullBath  BsmtHalfBath  FullBath  HalfBath  \\\n",
      "0     -0.126859   0.995878             1             0         2         1   \n",
      "1     -0.126859  -0.297242             0             0         1         1   \n",
      "2     -0.126859  -0.807986             1             0         1         0   \n",
      "3     -0.126859   6.076537             1             0         3         1   \n",
      "4     -0.126859   1.577399             1             0         3         0   \n",
      "\n",
      "   BedroomAbvGr  KitchenAbvGr  KitchenQual  TotRmsAbvGrd  Functional  \\\n",
      "0             3             1            0             7           6   \n",
      "1             3             1            3             4           6   \n",
      "2             2             2            3             3           6   \n",
      "3             3             1            0             9           6   \n",
      "4             4             1            0             9           6   \n",
      "\n",
      "   Fireplaces  GarageCars  GarageArea  PavedDrive  WoodDeckSF  OpenPorchSF  \\\n",
      "0           1           3    2.035714           2    0.496258     0.124767   \n",
      "1           1           0   -2.198715           2   -0.760062    -0.668511   \n",
      "2           2           1   -1.234206           2   -0.760062    -0.609750   \n",
      "3           1           3    1.960436           2    0.915031     5.295763   \n",
      "4           2           3    1.264107           2    0.423778    -0.360014   \n",
      "\n",
      "   EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal  MoSold  YrSold  \\\n",
      "0      -0.371011  -0.124694    -0.265635 -0.072618        0       8       3   \n",
      "1      -0.371011  -0.124694    -0.265635 -0.072618        0       4       3   \n",
      "2      -0.371011  -0.124694    -0.265635 -0.072618        0       0       2   \n",
      "3      -0.371011  -0.124694    -0.265635 -0.072618        0       9       1   \n",
      "4      -0.371011  -0.124694    -0.265635 -0.072618        0       6       0   \n",
      "\n",
      "   SaleType  SaleCondition  SalePrice  \n",
      "0         6              5   2.860721  \n",
      "1         8              4  -1.024943  \n",
      "2         8              4  -0.758158  \n",
      "3         6              5   0.089835  \n",
      "4         6              5   2.125653  \n",
      "Processed test data:\n",
      "   ID  MSSubClass  MSZoning   LotArea  Street  LotShape  LandContour  \\\n",
      "0   1           0         3  0.163874       1         3            3   \n",
      "1   2          12         4 -0.931739       1         3            3   \n",
      "2   3          14         3 -0.328214       1         3            3   \n",
      "3   4           5         3  3.021470       1         0            0   \n",
      "4   5           5         3  0.133529       1         0            3   \n",
      "\n",
      "   Utilities  LotConfig  LandSlope  Neighborhood  Condition1  Condition2  \\\n",
      "0          0          4          0            16           2           2   \n",
      "1          0          4          0            10           2           2   \n",
      "2          0          0          0             3           0           0   \n",
      "3          0          4          0             7           4           4   \n",
      "4          0          4          0            16           2           2   \n",
      "\n",
      "   BldgType  HouseStyle  OverallQual  OverallCond  YearBuilt  YearRemodAdd  \\\n",
      "0         0           2            9            4        110            59   \n",
      "1         3           5            3            5         71            58   \n",
      "2         1           1            4            5         42             0   \n",
      "3         0           5            9            4        108            58   \n",
      "4         0           5            8            4        106            56   \n",
      "\n",
      "   RoofStyle  RoofMatl  Exterior1st  Exterior2nd  MasVnrType  MasVnrArea  \\\n",
      "0          3         1            5            5           1    1.154729   \n",
      "1          1         1            5            5           3   -0.538598   \n",
      "2          1         1            8            8           3   -0.538598   \n",
      "3          3         1            5            5           2    3.691943   \n",
      "4          3         1           14           15           2    1.049243   \n",
      "\n",
      "   ExterQual  ExterCond  Foundation  BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  \\\n",
      "0          0          4           1    2.097434   -0.290871   0.057280   \n",
      "1          3          4           1   -0.948285   -0.290871   0.002742   \n",
      "2          3          4           0    0.856664   -0.290871  -0.919850   \n",
      "3          0          4           2    3.845117   -0.290871   0.757177   \n",
      "4          0          4           2    1.168447   -0.290871  -0.835771   \n",
      "\n",
      "   TotalBsmtSF  Heating  HeatingQC  CentralAir  Electrical  1stFlrSF  \\\n",
      "0     2.165902        1          0           1           4  2.209955   \n",
      "1    -1.104911        1          4           1           4 -0.914483   \n",
      "2    -0.107985        1          0           1           4 -0.201130   \n",
      "3     4.701903        1          0           1           4  5.068483   \n",
      "4     0.304227        1          0           1           4  0.067337   \n",
      "\n",
      "   2ndFlrSF  LowQualFinSF  GrLivArea  BsmtFullBath  BsmtHalfBath  FullBath  \\\n",
      "0 -0.788990     -0.126859   0.995878             1             0         2   \n",
      "1  0.487932     -0.126859  -0.297242             0             0         1   \n",
      "2 -0.788990     -0.126859  -0.807986             1             0         1   \n",
      "3  2.807907     -0.126859   6.076537             1             0         3   \n",
      "4  1.881788     -0.126859   1.577399             1             0         3   \n",
      "\n",
      "   HalfBath  BedroomAbvGr  KitchenAbvGr  KitchenQual  TotRmsAbvGrd  \\\n",
      "0         1             3             1            0             7   \n",
      "1         1             3             1            3             4   \n",
      "2         0             2             2            3             3   \n",
      "3         1             3             1            0             9   \n",
      "4         0             4             1            0             9   \n",
      "\n",
      "   Functional  Fireplaces  GarageCars  GarageArea  PavedDrive  WoodDeckSF  \\\n",
      "0           6           1           3    2.035714           2    0.496258   \n",
      "1           6           1           0   -2.198715           2   -0.760062   \n",
      "2           6           2           1   -1.234206           2   -0.760062   \n",
      "3           6           1           3    1.960436           2    0.915031   \n",
      "4           6           2           3    1.264107           2    0.423778   \n",
      "\n",
      "   OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal  \\\n",
      "0     0.124767      -0.371011  -0.124694    -0.265635 -0.072618        0   \n",
      "1    -0.668511      -0.371011  -0.124694    -0.265635 -0.072618        0   \n",
      "2    -0.609750      -0.371011  -0.124694    -0.265635 -0.072618        0   \n",
      "3     5.295763      -0.371011  -0.124694    -0.265635 -0.072618        0   \n",
      "4    -0.360014      -0.371011  -0.124694    -0.265635 -0.072618        0   \n",
      "\n",
      "   MoSold  YrSold  SaleType  SaleCondition  \n",
      "0       8       3         6              5  \n",
      "1       4       3         8              4  \n",
      "2       0       2         8              4  \n",
      "3       9       1         6              5  \n",
      "4       6       0         6              5  \n",
      "Train dataset size: 943\n",
      "Validation dataset size: 254\n",
      "Test dataset size: 254\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load the Data\n",
    "# ------------------------------\n",
    "# Adjust the file paths as needed.\n",
    "train_df = pd.read_csv(\"/Users/nicholasrichards/Desktop/machine_learning_2/ProblemSet2/house_train.csv\")\n",
    "val_df = pd.read_csv(\"/Users/nicholasrichards/Desktop/machine_learning_2/ProblemSet2/house_valid.csv\")\n",
    "# For the test set, note that it may not have the target column (\"SalePrice\")\n",
    "test_df = pd.read_csv(\"/Users/nicholasrichards/Desktop/machine_learning_2/ProblemSet2/house_test_noval.csv\")  \n",
    "\n",
    "# ------------------------------\n",
    "# 2. Identify Continuous and Categorical Features\n",
    "# ------------------------------\n",
    "# Continuous features: any column whose name includes \"area\", \"sf\", \"porch\", or \"saleprice\"\n",
    "continuous_cols = [col for col in train_df.columns if any(sub in col.lower() for sub in [\"area\", \"sf\", \"porch\", \"saleprice\"])]\n",
    "\n",
    "# The target is SalePrice; remove it from continuous features.\n",
    "if 'SalePrice' in continuous_cols:\n",
    "    continuous_cols.remove('SalePrice')\n",
    "\n",
    "# All other columns (except the target) are considered categorical.\n",
    "categorical_cols = [col for col in train_df.columns if col not in continuous_cols and col != \"SalePrice\"]\n",
    "\n",
    "print(\"Continuous columns:\", continuous_cols)\n",
    "print(\"Number of continuous columns:\", len(continuous_cols))\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "print(\"Number of categorical columns:\", len(categorical_cols))\n",
    "\n",
    "# For additional insight, print the number of distinct values for each categorical feature.\n",
    "distinct_values = {col: train_df[col].nunique() for col in categorical_cols}\n",
    "print(\"Distinct values per categorical variable:\")\n",
    "print(distinct_values)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Data Preprocessing: Standardization and Encoding\n",
    "# ------------------------------\n",
    "# Standardize continuous features.\n",
    "scaler = StandardScaler()\n",
    "train_df[continuous_cols] = scaler.fit_transform(train_df[continuous_cols])\n",
    "val_df[continuous_cols] = scaler.transform(val_df[continuous_cols])\n",
    "test_df[continuous_cols] = scaler.transform(test_df[continuous_cols])\n",
    "\n",
    "# Scale the target variable (SalePrice) separately.\n",
    "price_scaler = StandardScaler()\n",
    "train_df['SalePrice'] = price_scaler.fit_transform(train_df[['SalePrice']])\n",
    "val_df['SalePrice'] = price_scaler.transform(val_df[['SalePrice']])\n",
    "# The test set does not have a target.\n",
    "\n",
    "# Encode categorical variables using LabelEncoder.\n",
    "encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    train_df[col] = le.fit_transform(train_df[col])\n",
    "    val_df[col] = le.transform(val_df[col])\n",
    "    test_df[col] = le.transform(test_df[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "# Optionally, view a few rows of the processed data.\n",
    "print(\"Processed training data:\")\n",
    "print(train_df.head())\n",
    "print(\"Processed validation data:\")\n",
    "print(val_df.head())\n",
    "print(\"Processed test data:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Create a Custom Dataset for PyTorch\n",
    "# ------------------------------\n",
    "# We want our dataset to yield three items:\n",
    "#   - continuous features (float tensor)\n",
    "#   - categorical features (long tensor)\n",
    "#   - target (float tensor) for train/val; test data may not have a target.\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, continuous_cols, categorical_cols, target_col=None):\n",
    "        self.continuous = torch.tensor(df[continuous_cols].values, dtype=torch.float32)\n",
    "        self.categorical = torch.tensor(df[categorical_cols].values, dtype=torch.long)\n",
    "        if target_col is not None and target_col in df.columns:\n",
    "            self.target = torch.tensor(df[target_col].values, dtype=torch.float32)\n",
    "        else:\n",
    "            self.target = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.continuous)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.target is not None:\n",
    "            return self.continuous[idx], self.categorical[idx], self.target[idx]\n",
    "        else:\n",
    "            return self.continuous[idx], self.categorical[idx]\n",
    "\n",
    "# Create dataset objects.\n",
    "train_dataset = CustomDataset(train_df, continuous_cols, categorical_cols, target_col='SalePrice')\n",
    "val_dataset = CustomDataset(val_df, continuous_cols, categorical_cols, target_col='SalePrice')\n",
    "# For test data, omit the target.\n",
    "test_dataset = CustomDataset(test_df, continuous_cols, categorical_cols)\n",
    "\n",
    "# Create DataLoaders.\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "# For testing, use the full set or an appropriate batch size.\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_df.shape[0], shuffle=False)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 1.1594, Val Loss: 0.7715\n",
      "Epoch 2/100 - Train Loss: 0.4687, Val Loss: 0.4584\n",
      "Epoch 3/100 - Train Loss: 0.3313, Val Loss: 0.3662\n",
      "Epoch 4/100 - Train Loss: 0.2874, Val Loss: 0.3264\n",
      "Epoch 5/100 - Train Loss: 0.2582, Val Loss: 0.3203\n",
      "Epoch 6/100 - Train Loss: 0.2456, Val Loss: 0.2937\n",
      "Epoch 7/100 - Train Loss: 0.2288, Val Loss: 0.2834\n",
      "Epoch 8/100 - Train Loss: 0.2180, Val Loss: 0.2766\n",
      "Epoch 9/100 - Train Loss: 0.2127, Val Loss: 0.2698\n",
      "Epoch 10/100 - Train Loss: 0.2001, Val Loss: 0.2669\n",
      "Epoch 11/100 - Train Loss: 0.1957, Val Loss: 0.2620\n",
      "Epoch 12/100 - Train Loss: 0.1926, Val Loss: 0.2614\n",
      "Epoch 13/100 - Train Loss: 0.1832, Val Loss: 0.2614\n",
      "Epoch 14/100 - Train Loss: 0.1828, Val Loss: 0.2615\n",
      "Epoch 15/100 - Train Loss: 0.1760, Val Loss: 0.2594\n",
      "Epoch 16/100 - Train Loss: 0.1748, Val Loss: 0.2596\n",
      "Epoch 17/100 - Train Loss: 0.1807, Val Loss: 0.2606\n",
      "Epoch 18/100 - Train Loss: 0.1700, Val Loss: 0.2605\n",
      "Epoch 19/100 - Train Loss: 0.1728, Val Loss: 0.2570\n",
      "Epoch 20/100 - Train Loss: 0.1653, Val Loss: 0.2535\n",
      "Epoch 21/100 - Train Loss: 0.1602, Val Loss: 0.2517\n",
      "Epoch 22/100 - Train Loss: 0.1598, Val Loss: 0.2500\n",
      "Epoch 23/100 - Train Loss: 0.1528, Val Loss: 0.2475\n",
      "Epoch 24/100 - Train Loss: 0.1480, Val Loss: 0.2491\n",
      "Epoch 25/100 - Train Loss: 0.1447, Val Loss: 0.2470\n",
      "Epoch 26/100 - Train Loss: 0.1416, Val Loss: 0.2439\n",
      "Epoch 27/100 - Train Loss: 0.1410, Val Loss: 0.2417\n",
      "Epoch 28/100 - Train Loss: 0.1420, Val Loss: 0.2404\n",
      "Epoch 29/100 - Train Loss: 0.1376, Val Loss: 0.2424\n",
      "Epoch 30/100 - Train Loss: 0.1376, Val Loss: 0.2383\n",
      "Epoch 31/100 - Train Loss: 0.1306, Val Loss: 0.2354\n",
      "Epoch 32/100 - Train Loss: 0.1293, Val Loss: 0.2370\n",
      "Epoch 33/100 - Train Loss: 0.1286, Val Loss: 0.2355\n",
      "Epoch 34/100 - Train Loss: 0.1268, Val Loss: 0.2346\n",
      "Epoch 35/100 - Train Loss: 0.1256, Val Loss: 0.2360\n",
      "Epoch 36/100 - Train Loss: 0.1211, Val Loss: 0.2339\n",
      "Epoch 37/100 - Train Loss: 0.1205, Val Loss: 0.2310\n",
      "Epoch 38/100 - Train Loss: 0.1164, Val Loss: 0.2305\n",
      "Epoch 39/100 - Train Loss: 0.1186, Val Loss: 0.2279\n",
      "Epoch 40/100 - Train Loss: 0.1153, Val Loss: 0.2258\n",
      "Epoch 41/100 - Train Loss: 0.1125, Val Loss: 0.2273\n",
      "Epoch 42/100 - Train Loss: 0.1117, Val Loss: 0.2278\n",
      "Epoch 43/100 - Train Loss: 0.1125, Val Loss: 0.2254\n",
      "Epoch 44/100 - Train Loss: 0.1122, Val Loss: 0.2312\n",
      "Epoch 45/100 - Train Loss: 0.1074, Val Loss: 0.2254\n",
      "Epoch 46/100 - Train Loss: 0.1094, Val Loss: 0.2257\n",
      "Epoch 47/100 - Train Loss: 0.1125, Val Loss: 0.2274\n",
      "Epoch 48/100 - Train Loss: 0.1051, Val Loss: 0.2260\n",
      "Epoch 49/100 - Train Loss: 0.1074, Val Loss: 0.2226\n",
      "Epoch 50/100 - Train Loss: 0.1030, Val Loss: 0.2240\n",
      "Epoch 51/100 - Train Loss: 0.1009, Val Loss: 0.2251\n",
      "Epoch 52/100 - Train Loss: 0.1021, Val Loss: 0.2228\n",
      "Epoch 53/100 - Train Loss: 0.0996, Val Loss: 0.2249\n",
      "Epoch 54/100 - Train Loss: 0.1038, Val Loss: 0.2286\n",
      "Epoch 55/100 - Train Loss: 0.0979, Val Loss: 0.2231\n",
      "Epoch 56/100 - Train Loss: 0.0975, Val Loss: 0.2211\n",
      "Epoch 57/100 - Train Loss: 0.0996, Val Loss: 0.2284\n",
      "Epoch 58/100 - Train Loss: 0.0966, Val Loss: 0.2231\n",
      "Epoch 59/100 - Train Loss: 0.0943, Val Loss: 0.2221\n",
      "Epoch 60/100 - Train Loss: 0.0926, Val Loss: 0.2254\n",
      "Epoch 61/100 - Train Loss: 0.0926, Val Loss: 0.2223\n",
      "Epoch 62/100 - Train Loss: 0.0943, Val Loss: 0.2262\n",
      "Epoch 63/100 - Train Loss: 0.0897, Val Loss: 0.2246\n",
      "Epoch 64/100 - Train Loss: 0.0919, Val Loss: 0.2266\n",
      "Epoch 65/100 - Train Loss: 0.0914, Val Loss: 0.2264\n",
      "Epoch 66/100 - Train Loss: 0.0874, Val Loss: 0.2252\n",
      "Epoch 67/100 - Train Loss: 0.0883, Val Loss: 0.2241\n",
      "Epoch 68/100 - Train Loss: 0.0888, Val Loss: 0.2273\n",
      "Epoch 69/100 - Train Loss: 0.0868, Val Loss: 0.2235\n",
      "Epoch 70/100 - Train Loss: 0.0850, Val Loss: 0.2252\n",
      "Epoch 71/100 - Train Loss: 0.0858, Val Loss: 0.2246\n",
      "Epoch 72/100 - Train Loss: 0.0878, Val Loss: 0.2260\n",
      "Epoch 73/100 - Train Loss: 0.0867, Val Loss: 0.2264\n",
      "Epoch 74/100 - Train Loss: 0.0826, Val Loss: 0.2280\n",
      "Epoch 75/100 - Train Loss: 0.0842, Val Loss: 0.2300\n",
      "Epoch 76/100 - Train Loss: 0.0852, Val Loss: 0.2245\n",
      "Epoch 77/100 - Train Loss: 0.0818, Val Loss: 0.2256\n",
      "Epoch 78/100 - Train Loss: 0.0836, Val Loss: 0.2242\n",
      "Epoch 79/100 - Train Loss: 0.0797, Val Loss: 0.2279\n",
      "Epoch 80/100 - Train Loss: 0.0816, Val Loss: 0.2294\n",
      "Epoch 81/100 - Train Loss: 0.0793, Val Loss: 0.2285\n",
      "Epoch 82/100 - Train Loss: 0.0782, Val Loss: 0.2276\n",
      "Epoch 83/100 - Train Loss: 0.0802, Val Loss: 0.2279\n",
      "Epoch 84/100 - Train Loss: 0.0782, Val Loss: 0.2268\n",
      "Epoch 85/100 - Train Loss: 0.0785, Val Loss: 0.2232\n",
      "Epoch 86/100 - Train Loss: 0.0771, Val Loss: 0.2296\n",
      "Epoch 87/100 - Train Loss: 0.0757, Val Loss: 0.2267\n",
      "Epoch 88/100 - Train Loss: 0.0759, Val Loss: 0.2316\n",
      "Epoch 89/100 - Train Loss: 0.0766, Val Loss: 0.2278\n",
      "Epoch 90/100 - Train Loss: 0.0757, Val Loss: 0.2299\n",
      "Epoch 91/100 - Train Loss: 0.0742, Val Loss: 0.2280\n",
      "Epoch 92/100 - Train Loss: 0.0732, Val Loss: 0.2291\n",
      "Epoch 93/100 - Train Loss: 0.0740, Val Loss: 0.2296\n",
      "Epoch 94/100 - Train Loss: 0.0757, Val Loss: 0.2316\n",
      "Epoch 95/100 - Train Loss: 0.0761, Val Loss: 0.2383\n",
      "Epoch 96/100 - Train Loss: 0.0786, Val Loss: 0.2279\n",
      "Epoch 97/100 - Train Loss: 0.0730, Val Loss: 0.2297\n",
      "Epoch 98/100 - Train Loss: 0.0705, Val Loss: 0.2306\n",
      "Epoch 99/100 - Train Loss: 0.0732, Val Loss: 0.2298\n",
      "Epoch 100/100 - Train Loss: 0.0739, Val Loss: 0.2299\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming train_loader, val_loader, and test_loader are already defined from your custom dataset\n",
    "# and that continuous_cols and categorical_cols are defined as in your preprocessing step.\n",
    "\n",
    "# Compute the number of unique categories for each categorical column\n",
    "num_categories = [train_df[col].nunique() for col in categorical_cols]\n",
    "# For our basic linear model, we use a 1-dimensional embedding for each categorical feature.\n",
    "embedding_dims = [1] * len(num_categories)\n",
    "\n",
    "# Define the basic linear regression model with embeddings\n",
    "class BasicLinearRegressionWithEmbeddings(nn.Module):\n",
    "    def __init__(self, num_continuous, num_categories, embedding_dims):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_continuous (int): Number of continuous features.\n",
    "            num_categories (list of int): List of unique counts for each categorical feature.\n",
    "            embedding_dims (list of int): List of embedding dimensions for each categorical feature.\n",
    "        \"\"\"\n",
    "        super(BasicLinearRegressionWithEmbeddings, self).__init__()\n",
    "        # Create an embedding layer for each categorical feature.\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(num_categories[i], embedding_dims[i]) for i in range(len(num_categories))\n",
    "        ])\n",
    "        # Total embedding dimension (sum over all categorical features)\n",
    "        total_embedding_dim = sum(embedding_dims)\n",
    "        # The input dimension for the final linear layer\n",
    "        input_dim = num_continuous + total_embedding_dim\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, continuous, categorical):\n",
    "        # Process each categorical feature with its embedding layer.\n",
    "        embedded = [emb(categorical[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        # Each embedded output has shape [batch_size, embedding_dim]; concatenate along dimension 1.\n",
    "        embedded = torch.cat(embedded, dim=1)\n",
    "        # Concatenate continuous features with the embedded categorical features.\n",
    "        x = torch.cat([continuous, embedded], dim=1)\n",
    "        # Compute the linear output.\n",
    "        out = self.linear(x)\n",
    "        return out.squeeze(1)  # Squeeze to return a 1D tensor of predictions.\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create an instance of the model.\n",
    "model = BasicLinearRegressionWithEmbeddings(\n",
    "    num_continuous=len(continuous_cols),\n",
    "    num_categories=num_categories,\n",
    "    embedding_dims=embedding_dims\n",
    ").to(device)\n",
    "\n",
    "# Define loss function and optimizer.\n",
    "criterion = nn.MSELoss()  # Baseline loss; later you can experiment with others.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Training loop.\n",
    "num_epochs = 100  # You can adjust this as needed.\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for batch in train_loader:\n",
    "        # Each batch is a tuple: (continuous, categorical, target)\n",
    "        continuous, categorical, target = batch\n",
    "        continuous = continuous.to(device)\n",
    "        categorical = categorical.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(continuous, categorical)\n",
    "        loss = criterion(predictions, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "    \n",
    "    # Evaluate on the validation set.\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            continuous, categorical, target = batch\n",
    "            continuous = continuous.to(device)\n",
    "            categorical = categorical.to(device)\n",
    "            target = target.to(device)\n",
    "            predictions = model(continuous, categorical)\n",
    "            loss = criterion(predictions, target)\n",
    "            val_losses.append(loss.item())\n",
    "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type                                | Params | Mode \n",
      "--------------------------------------------------------------------------\n",
      "0 | model     | BasicLinearRegressionWithEmbeddings | 547    | train\n",
      "1 | criterion | MSELoss                             | 0      | train\n",
      "--------------------------------------------------------------------------\n",
      "547       Trainable params\n",
      "0         Non-trainable params\n",
      "547       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "50        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 15/15 [00:00<00:00, 28.93it/s, v_num=1, val_loss=0.250, train_loss=0.138]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 15/15 [00:00<00:00, 27.11it/s, v_num=1, val_loss=0.250, train_loss=0.138]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# ------------------------------\n",
    "# Your original basic model definition\n",
    "# ------------------------------\n",
    "class BasicLinearRegressionWithEmbeddings(nn.Module):\n",
    "    def __init__(self, num_continuous, num_categories, embedding_dims):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_continuous (int): Number of continuous features.\n",
    "            num_categories (list of int): List of unique counts for each categorical feature.\n",
    "            embedding_dims (list of int): List of embedding dimensions for each categorical feature.\n",
    "        \"\"\"\n",
    "        super(BasicLinearRegressionWithEmbeddings, self).__init__()\n",
    "        # Create an embedding layer for each categorical feature.\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(num_categories[i], embedding_dims[i]) for i in range(len(num_categories))\n",
    "        ])\n",
    "        # Total embedding dimension (sum over all categorical features)\n",
    "        total_embedding_dim = sum(embedding_dims)\n",
    "        # The input dimension for the final linear layer is continuous features plus embeddings.\n",
    "        input_dim = num_continuous + total_embedding_dim\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, continuous, categorical):\n",
    "        # Process each categorical feature with its embedding layer.\n",
    "        embedded = [emb(categorical[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        # Concatenate all embeddings (each of shape [batch_size, embedding_dim]).\n",
    "        embedded = torch.cat(embedded, dim=1)\n",
    "        # Concatenate continuous features with the embeddings.\n",
    "        x = torch.cat([continuous, embedded], dim=1)\n",
    "        # Compute the linear output.\n",
    "        out = self.linear(x)\n",
    "        return out.squeeze(1)  # Return a 1D tensor of predictions.\n",
    "\n",
    "# ------------------------------\n",
    "# Convert the above model into a Lightning Module\n",
    "# ------------------------------\n",
    "class BasicLinearRegressionLightning(pl.LightningModule):\n",
    "    def __init__(self, num_continuous, num_categories, embedding_dims, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_continuous (int): Number of continuous features.\n",
    "            num_categories (list of int): List of unique counts for each categorical feature.\n",
    "            embedding_dims (list of int): List of embedding dimensions for each categorical feature.\n",
    "            learning_rate (float): Learning rate for the optimizer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()  # Saves all arguments to self.hparams\n",
    "        self.model = BasicLinearRegressionWithEmbeddings(\n",
    "            num_continuous=num_continuous,\n",
    "            num_categories=num_categories,\n",
    "            embedding_dims=embedding_dims\n",
    "        )\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, continuous, categorical):\n",
    "        return self.model(continuous, categorical)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Expecting batch as a tuple: (continuous, categorical, target)\n",
    "        continuous, categorical, target = batch\n",
    "        preds = self(continuous, categorical)\n",
    "        loss = self.criterion(preds, target)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        continuous, categorical, target = batch\n",
    "        preds = self(continuous, categorical)\n",
    "        loss = self.criterion(preds, target)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    '''\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Try to unpack three values; if that fails, unpack two.\n",
    "        try:\n",
    "            continuous, categorical, target = batch\n",
    "        except ValueError:\n",
    "            continuous, categorical = batch\n",
    "        preds = self(continuous, categorical)\n",
    "        # Optionally log loss if target is present.\n",
    "        try:\n",
    "            loss = self.criterion(preds, target)\n",
    "            self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n",
    "        except NameError:\n",
    "            pass  # No target provided in the test batch.\n",
    "        return preds\n",
    "    '''\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "# ------------------------------\n",
    "# Usage example (assuming your DataLoaders and feature lists are defined)\n",
    "# ------------------------------\n",
    "# Compute the number of unique categories for each categorical column\n",
    "num_categories = [train_df[col].nunique() for col in categorical_cols]\n",
    "# For our basic model, we use a 1-dimensional embedding for each categorical feature.\n",
    "embedding_dims = [1] * len(num_categories)\n",
    "\n",
    "# Instantiate the Lightning model.\n",
    "model_lightning = BasicLinearRegressionLightning(\n",
    "    num_continuous=len(continuous_cols),\n",
    "    num_categories=num_categories,\n",
    "    embedding_dims=embedding_dims,\n",
    "    learning_rate=0.001  # or any other value you'd like to use\n",
    ")\n",
    "\n",
    "# Now you can pass this model to a PyTorch Lightning Trainer:\n",
    "trainer = pl.Trainer(max_epochs=100, accelerator='auto', devices='auto')\n",
    "trainer.fit(model_lightning, train_dataloaders=train_loader, val_dataloaders=valid_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "sample = test_dataset[0]\n",
    "print(len(sample))  # Should print 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to basic_reg_preds.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# --- Step 1: Extract IDs from the original test DataFrame ---\n",
    "test_ids = test_df[\"ID\"]\n",
    "\n",
    "# --- Step 2: Prepare the features for prediction ---\n",
    "test_features = test_df.drop(columns=[\"ID\"])\n",
    "test_continuous = test_features[continuous_cols].values\n",
    "test_categorical = test_features[categorical_cols].values\n",
    "\n",
    "# --- Step 3: Convert features to PyTorch tensors ---\n",
    "test_continuous_tensor = torch.tensor(test_continuous, dtype=torch.float32)\n",
    "test_categorical_tensor = torch.tensor(test_categorical, dtype=torch.long)\n",
    "\n",
    "# --- Step 4: Run predictions ---\n",
    "# Ensure your trained model is loaded as model_lightning.\n",
    "model_lightning.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model_lightning(test_continuous_tensor.to(model_lightning.device),\n",
    "                            test_categorical_tensor.to(model_lightning.device))\n",
    "    preds = preds.cpu().numpy()\n",
    "\n",
    "# --- Step 5: Inverse-transform the predictions ---\n",
    "# Assuming price_scaler was fitted on the training targets (SalePrice)\n",
    "# The scaler expects a 2D array, so we reshape and then squeeze after inverse transform.\n",
    "preds_unscaled = price_scaler.inverse_transform(preds.reshape(-1, 1)).squeeze()\n",
    "\n",
    "# --- Step 6: Create output DataFrame ---\n",
    "output_df = pd.DataFrame({\n",
    "    \"ID\": test_ids,\n",
    "    \"Prediction\": preds_unscaled\n",
    "})\n",
    "\n",
    "# Format predictions to 4 decimal places (if desired).\n",
    "output_df[\"Prediction\"] = output_df[\"Prediction\"].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "# --- Step 7: Save the DataFrame to a CSV file ---\n",
    "output_df.to_csv(\"basic_reg_preds.csv\", index=False)\n",
    "print(\"Predictions saved to basic_reg_preds.csv\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # For heteroskedastic models, the output shape is [N, 2] (mean and log variance).\n",
    "    # We take the first output (the predicted mean) as the prediction.\n",
    "    preds = model_lightning(test_continuous_tensor.to(model_lightning.device),\n",
    "                            test_categorical_tensor.to(model_lightning.device))\n",
    "    if preds.ndim == 2 and preds.shape[1] == 2:\n",
    "        preds = preds[:, 0]\n",
    "    preds = preds.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are implementing hidden layers which builds on our linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 0.5516, Val Loss: 0.6300\n",
      "Epoch 2/100 - Train Loss: 0.2343, Val Loss: 0.3530\n",
      "Epoch 3/100 - Train Loss: 0.2118, Val Loss: 0.3158\n",
      "Epoch 4/100 - Train Loss: 0.1340, Val Loss: 0.3202\n",
      "Epoch 5/100 - Train Loss: 0.1219, Val Loss: 0.3156\n",
      "Epoch 6/100 - Train Loss: 0.0947, Val Loss: 0.3113\n",
      "Epoch 7/100 - Train Loss: 0.0847, Val Loss: 0.3125\n",
      "Epoch 8/100 - Train Loss: 0.0943, Val Loss: 0.3045\n",
      "Epoch 9/100 - Train Loss: 0.0898, Val Loss: 0.3216\n",
      "Epoch 10/100 - Train Loss: 0.0750, Val Loss: 0.3371\n",
      "Epoch 11/100 - Train Loss: 0.0642, Val Loss: 0.3237\n",
      "Epoch 12/100 - Train Loss: 0.0560, Val Loss: 0.3176\n",
      "Epoch 13/100 - Train Loss: 0.0581, Val Loss: 0.3258\n",
      "Epoch 14/100 - Train Loss: 0.0614, Val Loss: 0.3322\n",
      "Epoch 15/100 - Train Loss: 0.0505, Val Loss: 0.3261\n",
      "Epoch 16/100 - Train Loss: 0.0449, Val Loss: 0.3230\n",
      "Epoch 17/100 - Train Loss: 0.0509, Val Loss: 0.3276\n",
      "Epoch 18/100 - Train Loss: 0.0671, Val Loss: 0.3231\n",
      "Epoch 19/100 - Train Loss: 0.0481, Val Loss: 0.3293\n",
      "Epoch 20/100 - Train Loss: 0.0483, Val Loss: 0.3054\n",
      "Epoch 21/100 - Train Loss: 0.0526, Val Loss: 0.3026\n",
      "Epoch 22/100 - Train Loss: 0.0481, Val Loss: 0.2836\n",
      "Epoch 23/100 - Train Loss: 0.0550, Val Loss: 0.3042\n",
      "Epoch 24/100 - Train Loss: 0.0483, Val Loss: 0.3036\n",
      "Epoch 25/100 - Train Loss: 0.0491, Val Loss: 0.3217\n",
      "Epoch 26/100 - Train Loss: 0.0551, Val Loss: 0.3027\n",
      "Epoch 27/100 - Train Loss: 0.0435, Val Loss: 0.2947\n",
      "Epoch 28/100 - Train Loss: 0.0522, Val Loss: 0.2822\n",
      "Epoch 29/100 - Train Loss: 0.0470, Val Loss: 0.2843\n",
      "Epoch 30/100 - Train Loss: 0.0463, Val Loss: 0.2892\n",
      "Epoch 31/100 - Train Loss: 0.0452, Val Loss: 0.2955\n",
      "Epoch 32/100 - Train Loss: 0.0480, Val Loss: 0.2927\n",
      "Epoch 33/100 - Train Loss: 0.0484, Val Loss: 0.2845\n",
      "Epoch 34/100 - Train Loss: 0.0390, Val Loss: 0.2823\n",
      "Epoch 35/100 - Train Loss: 0.0486, Val Loss: 0.2750\n",
      "Epoch 36/100 - Train Loss: 0.0391, Val Loss: 0.2843\n",
      "Epoch 37/100 - Train Loss: 0.0366, Val Loss: 0.2664\n",
      "Epoch 38/100 - Train Loss: 0.0338, Val Loss: 0.2670\n",
      "Epoch 39/100 - Train Loss: 0.0350, Val Loss: 0.2736\n",
      "Epoch 40/100 - Train Loss: 0.0406, Val Loss: 0.2785\n",
      "Epoch 41/100 - Train Loss: 0.0371, Val Loss: 0.2699\n",
      "Epoch 42/100 - Train Loss: 0.0448, Val Loss: 0.2754\n",
      "Epoch 43/100 - Train Loss: 0.0370, Val Loss: 0.2820\n",
      "Epoch 44/100 - Train Loss: 0.0396, Val Loss: 0.2806\n",
      "Epoch 45/100 - Train Loss: 0.0285, Val Loss: 0.2713\n",
      "Epoch 46/100 - Train Loss: 0.0416, Val Loss: 0.2714\n",
      "Epoch 47/100 - Train Loss: 0.0446, Val Loss: 0.2723\n",
      "Epoch 48/100 - Train Loss: 0.0299, Val Loss: 0.2871\n",
      "Epoch 49/100 - Train Loss: 0.0340, Val Loss: 0.2744\n",
      "Epoch 50/100 - Train Loss: 0.0316, Val Loss: 0.2892\n",
      "Epoch 51/100 - Train Loss: 0.0211, Val Loss: 0.2839\n",
      "Epoch 52/100 - Train Loss: 0.0232, Val Loss: 0.2861\n",
      "Epoch 53/100 - Train Loss: 0.0291, Val Loss: 0.2797\n",
      "Epoch 54/100 - Train Loss: 0.0302, Val Loss: 0.2795\n",
      "Epoch 55/100 - Train Loss: 0.0269, Val Loss: 0.2832\n",
      "Epoch 56/100 - Train Loss: 0.0335, Val Loss: 0.2740\n",
      "Epoch 57/100 - Train Loss: 0.0456, Val Loss: 0.2695\n",
      "Epoch 58/100 - Train Loss: 0.0354, Val Loss: 0.2654\n",
      "Epoch 59/100 - Train Loss: 0.0452, Val Loss: 0.2805\n",
      "Epoch 60/100 - Train Loss: 0.0462, Val Loss: 0.2691\n",
      "Epoch 61/100 - Train Loss: 0.0398, Val Loss: 0.2710\n",
      "Epoch 62/100 - Train Loss: 0.0276, Val Loss: 0.2619\n",
      "Epoch 63/100 - Train Loss: 0.0292, Val Loss: 0.2610\n",
      "Epoch 64/100 - Train Loss: 0.0333, Val Loss: 0.2643\n",
      "Epoch 65/100 - Train Loss: 0.0434, Val Loss: 0.2661\n",
      "Epoch 66/100 - Train Loss: 0.0260, Val Loss: 0.2697\n",
      "Epoch 67/100 - Train Loss: 0.0370, Val Loss: 0.2577\n",
      "Epoch 68/100 - Train Loss: 0.0289, Val Loss: 0.2538\n",
      "Epoch 69/100 - Train Loss: 0.0240, Val Loss: 0.2528\n",
      "Epoch 70/100 - Train Loss: 0.0370, Val Loss: 0.2666\n",
      "Epoch 71/100 - Train Loss: 0.0369, Val Loss: 0.2603\n",
      "Epoch 72/100 - Train Loss: 0.0276, Val Loss: 0.2644\n",
      "Epoch 73/100 - Train Loss: 0.0406, Val Loss: 0.2576\n",
      "Epoch 74/100 - Train Loss: 0.0216, Val Loss: 0.2577\n",
      "Epoch 75/100 - Train Loss: 0.0297, Val Loss: 0.2545\n",
      "Epoch 76/100 - Train Loss: 0.0321, Val Loss: 0.2586\n",
      "Epoch 77/100 - Train Loss: 0.0228, Val Loss: 0.2598\n",
      "Epoch 78/100 - Train Loss: 0.0302, Val Loss: 0.2583\n",
      "Epoch 79/100 - Train Loss: 0.0327, Val Loss: 0.2548\n",
      "Epoch 80/100 - Train Loss: 0.0270, Val Loss: 0.2602\n",
      "Epoch 81/100 - Train Loss: 0.0465, Val Loss: 0.2568\n",
      "Epoch 82/100 - Train Loss: 0.0193, Val Loss: 0.2570\n",
      "Epoch 83/100 - Train Loss: 0.0260, Val Loss: 0.2643\n",
      "Epoch 84/100 - Train Loss: 0.0242, Val Loss: 0.2529\n",
      "Epoch 85/100 - Train Loss: 0.0356, Val Loss: 0.2681\n",
      "Epoch 86/100 - Train Loss: 0.0301, Val Loss: 0.2606\n",
      "Epoch 87/100 - Train Loss: 0.0283, Val Loss: 0.2641\n",
      "Epoch 88/100 - Train Loss: 0.0426, Val Loss: 0.2544\n",
      "Epoch 89/100 - Train Loss: 0.0287, Val Loss: 0.2516\n",
      "Epoch 90/100 - Train Loss: 0.0283, Val Loss: 0.2615\n",
      "Epoch 91/100 - Train Loss: 0.0316, Val Loss: 0.2584\n",
      "Epoch 92/100 - Train Loss: 0.0210, Val Loss: 0.2634\n",
      "Epoch 93/100 - Train Loss: 0.0222, Val Loss: 0.2686\n",
      "Epoch 94/100 - Train Loss: 0.0307, Val Loss: 0.2586\n",
      "Epoch 95/100 - Train Loss: 0.0292, Val Loss: 0.2546\n",
      "Epoch 96/100 - Train Loss: 0.0405, Val Loss: 0.2571\n",
      "Epoch 97/100 - Train Loss: 0.0343, Val Loss: 0.2601\n",
      "Epoch 98/100 - Train Loss: 0.0267, Val Loss: 0.2593\n",
      "Epoch 99/100 - Train Loss: 0.0290, Val Loss: 0.2611\n",
      "Epoch 100/100 - Train Loss: 0.0292, Val Loss: 0.2565\n",
      "Test set predictions (first 10): [ 1.8578825  -0.75138646 -0.61135226  4.7184896   2.4244227   0.7872605\n",
      "  0.6692882   3.281457   -1.017698    2.9324257 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the deep regression model with embeddings and hidden layers.\n",
    "class DeepRegressionWithEmbeddings(nn.Module):\n",
    "    def __init__(self, num_continuous, num_categories, embedding_dims, hidden_units=[128, 64]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_continuous (int): Number of continuous features.\n",
    "            num_categories (list of int): Unique category counts for each categorical variable.\n",
    "            embedding_dims (list of int): Embedding dimensions for each categorical variable.\n",
    "            hidden_units (list of int): List specifying the number of units in each hidden layer.\n",
    "        \"\"\"\n",
    "        super(DeepRegressionWithEmbeddings, self).__init__()\n",
    "        # Create embedding layers for each categorical feature.\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(num_categories[i], embedding_dims[i]) for i in range(len(num_categories))\n",
    "        ])\n",
    "        total_embedding_dim = sum(embedding_dims)\n",
    "        # The input dimension is the sum of continuous features and all embeddings.\n",
    "        input_dim = num_continuous + total_embedding_dim\n",
    "        \n",
    "        # Build the hidden layers.\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for units in hidden_units:\n",
    "            layers.append(nn.Linear(prev_dim, units))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(units))\n",
    "            prev_dim = units\n",
    "        # Final output layer.\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        # Combine layers into a sequential model.\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, continuous, categorical):\n",
    "        # Process each categorical feature through its embedding.\n",
    "        embedded = [emb(categorical[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        # Concatenate all embedded categorical features.\n",
    "        embedded = torch.cat(embedded, dim=1)\n",
    "        # Concatenate continuous features and embeddings.\n",
    "        x = torch.cat([continuous, embedded], dim=1)\n",
    "        # Forward pass through the hidden layers and output layer.\n",
    "        out = self.model(x)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "# Device configuration.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume continuous_cols and categorical_cols have been defined during preprocessing.\n",
    "# Compute the number of unique categories for each categorical column.\n",
    "num_categories = [train_df[col].nunique() for col in categorical_cols]\n",
    "# For this deep model, we still use 1-dimensional embeddings.\n",
    "embedding_dims = [1] * len(num_categories)\n",
    "\n",
    "# Instantiate the model.\n",
    "deep_model = DeepRegressionWithEmbeddings(\n",
    "    num_continuous=len(continuous_cols),\n",
    "    num_categories=num_categories,\n",
    "    embedding_dims=embedding_dims,\n",
    "    hidden_units=[128, 64]  # You can experiment with different hidden layer sizes.\n",
    ").to(device)\n",
    "\n",
    "# Define loss function and optimizer.\n",
    "criterion = nn.MSELoss()  # Baseline loss; later, you can experiment with alternative loss functions.\n",
    "optimizer = torch.optim.Adam(deep_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop.\n",
    "num_epochs = 100  # Adjust the number of epochs as needed.\n",
    "for epoch in range(num_epochs):\n",
    "    deep_model.train()\n",
    "    train_losses = []\n",
    "    for batch in train_loader:\n",
    "        # Each batch: (continuous, categorical, target)\n",
    "        continuous, categorical, target = batch\n",
    "        continuous = continuous.to(device)\n",
    "        categorical = categorical.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = deep_model(continuous, categorical)\n",
    "        loss = criterion(predictions, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "    \n",
    "    # Evaluate on the validation set.\n",
    "    deep_model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            continuous, categorical, target = batch\n",
    "            continuous = continuous.to(device)\n",
    "            categorical = categorical.to(device)\n",
    "            target = target.to(device)\n",
    "            preds = deep_model(continuous, categorical)\n",
    "            loss = criterion(preds, target)\n",
    "            val_losses.append(loss.item())\n",
    "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Generate predictions for the test set.\n",
    "deep_model.eval()\n",
    "test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Test dataset returns only continuous and categorical features.\n",
    "        continuous, categorical = batch\n",
    "        continuous = continuous.to(device)\n",
    "        categorical = categorical.to(device)\n",
    "        preds = deep_model(continuous, categorical)\n",
    "        test_predictions.append(preds.cpu().numpy())\n",
    "test_predictions = np.concatenate(test_predictions)\n",
    "print(\"Test set predictions (first 10):\", test_predictions[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe lots of overfitting in this deep linear regression model, so we will start trying out some weight decay and dropout layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.2958 - Val Loss: 0.3714\n",
      "  New best model found!\n",
      "Epoch 2/50 - Train Loss: 0.2538 - Val Loss: 0.3933\n",
      "  No improvement for 1 epochs.\n",
      "Epoch 3/50 - Train Loss: 0.2089 - Val Loss: 0.2763\n",
      "  New best model found!\n",
      "Epoch 4/50 - Train Loss: 0.1753 - Val Loss: 0.3226\n",
      "  No improvement for 1 epochs.\n",
      "Epoch 5/50 - Train Loss: 0.1633 - Val Loss: 0.3834\n",
      "  No improvement for 2 epochs.\n",
      "Epoch 6/50 - Train Loss: 0.2232 - Val Loss: 0.4471\n",
      "  No improvement for 3 epochs.\n",
      "Epoch 7/50 - Train Loss: 0.2016 - Val Loss: 0.3230\n",
      "  No improvement for 4 epochs.\n",
      "Epoch 8/50 - Train Loss: 0.1772 - Val Loss: 0.3389\n",
      "  No improvement for 5 epochs.\n",
      "Epoch 9/50 - Train Loss: 0.1471 - Val Loss: 0.3566\n",
      "  No improvement for 6 epochs.\n",
      "Epoch 10/50 - Train Loss: 0.1374 - Val Loss: 0.4336\n",
      "  No improvement for 7 epochs.\n",
      "Epoch 11/50 - Train Loss: 0.1959 - Val Loss: 0.3020\n",
      "  No improvement for 8 epochs.\n",
      "Epoch 12/50 - Train Loss: 0.2006 - Val Loss: 0.3020\n",
      "  No improvement for 9 epochs.\n",
      "Epoch 13/50 - Train Loss: 0.1773 - Val Loss: 0.3033\n",
      "  No improvement for 10 epochs.\n",
      "Early stopping triggered.\n",
      "Restored best model with validation loss 0.2763\n",
      "Test set predictions (first 10): [ 1.9035852  -0.97950155 -0.59366584  4.502553    2.0132961   0.5603528\n",
      "  0.75638956  2.3843532  -0.5922802   2.2803903 ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assume deep_model, train_loader, val_loader, and test_loader have already been defined.\n",
    "# Also assume device is set to \"cuda\" if available.\n",
    "\n",
    "# Define loss function.\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create an optimizer with weight decay for L2 regularization.\n",
    "optimizer = torch.optim.Adam(deep_model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "\n",
    "# Early stopping parameters.\n",
    "patience = 10            # Number of epochs to wait for improvement.\n",
    "best_val_loss = np.inf  # Initialize best validation loss as infinity.\n",
    "epochs_without_improvement = 0\n",
    "best_model_state = None\n",
    "\n",
    "num_epochs = 50  # Maximum number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    deep_model.train()\n",
    "    train_losses = []\n",
    "    for batch in train_loader:\n",
    "        # Each batch: (continuous, categorical, target)\n",
    "        continuous, categorical, target = batch\n",
    "        continuous = continuous.to(device)\n",
    "        categorical = categorical.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = deep_model(continuous, categorical)\n",
    "        loss = criterion(predictions, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "    \n",
    "    # Evaluate on validation set.\n",
    "    deep_model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            continuous, categorical, target = batch\n",
    "            continuous = continuous.to(device)\n",
    "            categorical = categorical.to(device)\n",
    "            target = target.to(device)\n",
    "            preds = deep_model(continuous, categorical)\n",
    "            loss = criterion(preds, target)\n",
    "            val_losses.append(loss.item())\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Check for improvement.\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        best_model_state = deep_model.state_dict()\n",
    "        print(\"  New best model found!\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"  No improvement for {epochs_without_improvement} epochs.\")\n",
    "    \n",
    "    # Trigger early stopping if no improvement for 'patience' epochs.\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Restore the best model state.\n",
    "if best_model_state is not None:\n",
    "    deep_model.load_state_dict(best_model_state)\n",
    "    print(f\"Restored best model with validation loss {best_val_loss:.4f}\")\n",
    "\n",
    "# Generate predictions on the test set.\n",
    "deep_model.eval()\n",
    "test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Test dataset yields only continuous and categorical features.\n",
    "        continuous, categorical = batch\n",
    "        continuous = continuous.to(device)\n",
    "        categorical = categorical.to(device)\n",
    "        preds = deep_model(continuous, categorical)\n",
    "        test_predictions.append(preds.cpu().numpy())\n",
    "test_predictions = np.concatenate(test_predictions)\n",
    "print(\"Test set predictions (first 10):\", test_predictions[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to focus on Huber loss for this problem because it is symmetric around zero, meaning that it treats positive and negative values equally, and weights larger variations from the mean less than more closer ones. This is ideal for analyzing our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class HyperparameterLogger(pl.Callback):\n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        # Create a dictionary with the hyperparameters you want to log.\n",
    "        hparams = pl_module.hparams\n",
    "        # For example, assume you want to log learning_rate, weight_decay, and hidden_units.\n",
    "        metrics = {\n",
    "            \"learning_rate\": hparams.learning_rate,\n",
    "            \"weight_decay\": hparams.weight_decay if hasattr(hparams, \"weight_decay\") else None,\n",
    "            \"hidden_units\": str(getattr(hparams, \"hidden_units\", \"N/A\"))\n",
    "        }\n",
    "        # Log these metrics at step 0. They will appear in your CSV log.\n",
    "        trainer.logger.log_metrics(metrics, step=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After experimenting with many different variations with our hyperparamters of our deep regression model, we found using a learning rate of 0.05, weight decay of 0.0005, two hidden layers with values 512 and 256, and early stopping with patience of 10 epochs. These parameters allowed us to minimize Huber loss while maintaining relatively low values for MSE and MAE loss. We ended up using 50 epochs as this helped us run our code more efficiently and seemed to eliminate any overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                         </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model      │ DeepRegressionWithEmbeddings │  166 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ mse_loss   │ MSELoss                      │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ mae_loss   │ L1Loss                       │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ huber_loss │ SmoothL1Loss                 │      0 │ train │\n",
       "└───┴────────────┴──────────────────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                        \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model      │ DeepRegressionWithEmbeddings │  166 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ mse_loss   │ MSELoss                      │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ mae_loss   │ L1Loss                       │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ huber_loss │ SmoothL1Loss                 │      0 │ train │\n",
       "└───┴────────────┴──────────────────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 166 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 166 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 59                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 166 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 166 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 59                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/trainer/co\n",
       "nnectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider\n",
       "increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/trainer/co\n",
       "nnectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider\n",
       "increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/trainer/co\n",
       "nnectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. \n",
       "Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve \n",
       "performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/trainer/co\n",
       "nnectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. \n",
       "Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve \n",
       "performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/loops/fit_\n",
       "loop.py:298: The number of training batches (15) is smaller than the logging interval \n",
       "Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training \n",
       "epoch.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/loops/fit_\n",
       "loop.py:298: The number of training batches (15) is smaller than the logging interval \n",
       "Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training \n",
       "epoch.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Define the Lightning Module that wraps the deep regression model.\n",
    "# -----------------------------------------------------------------------------\n",
    "class DeepRegressionLightning(pl.LightningModule):\n",
    "    def __init__(self, num_continuous, num_categories, embedding_dims, learning_rate=0.05):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_continuous (int): Number of continuous features.\n",
    "            num_categories (list of int): Unique counts for each categorical feature.\n",
    "            embedding_dims (list of int): Embedding dimensions for each categorical feature.\n",
    "            learning_rate (float): Learning rate for the optimizer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()  # This will save num_continuous, num_categories, embedding_dims, and learning_rate\n",
    "\n",
    "        # Instantiate your deep regression model.\n",
    "        # Here we use a two-hidden-layer architecture as an example.\n",
    "        self.model = DeepRegressionWithEmbeddings(\n",
    "            num_continuous=num_continuous,\n",
    "            num_categories=num_categories,\n",
    "            embedding_dims=embedding_dims,\n",
    "            hidden_units=[512, 256]\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Define multiple loss functions.\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.mae_loss = nn.L1Loss()\n",
    "        self.huber_loss = nn.SmoothL1Loss()  # Huber Loss\n",
    "        \n",
    "    def forward(self, continuous, categorical):\n",
    "        return self.model(continuous, categorical)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        continuous, categorical, target = batch\n",
    "        preds = self(continuous, categorical)\n",
    "        loss_mse = self.mse_loss(preds, target)\n",
    "        loss_mae = self.mae_loss(preds, target)\n",
    "        loss_huber = self.huber_loss(preds, target)\n",
    "        \n",
    "        # Log the three loss metrics for training.\n",
    "        self.log(\"train_loss_mse\", loss_mse, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_loss_mae\", loss_mae, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_loss_huber\", loss_huber, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        # Use MSE as the main loss to backpropagate.\n",
    "        return loss_huber\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        continuous, categorical, target = batch\n",
    "        preds = self(continuous, categorical)\n",
    "        loss_mse = self.mse_loss(preds, target)\n",
    "        loss_mae = self.mae_loss(preds, target)\n",
    "        loss_huber = self.huber_loss(preds, target)\n",
    "        \n",
    "        # Log the validation metrics.\n",
    "        self.log(\"val_loss_mse\", loss_mse, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_loss_mae\", loss_mae, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_loss_huber\", loss_huber, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss_huber\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        continuous, categorical, target = batch\n",
    "        preds = self(continuous, categorical)\n",
    "        loss_mse = self.mse_loss(preds, target)\n",
    "        self.log(\"test_loss_mse\", loss_mse, on_step=False, on_epoch=True)\n",
    "        return loss_huber\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Using weight decay (L2 regularization) as well.\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=0.0005)\n",
    "        return optimizer\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CSVLogger, ModelCheckpoint, and RichProgressBar Setup\n",
    "# -----------------------------------------------------------------------------\n",
    "folder_name = \"Model5\"\n",
    "\n",
    "# CSVLogger will save logs in \"PS2Q3/Model1\".\n",
    "logger = CSVLogger(\"PS2Q3\", name=folder_name, flush_logs_every_n_steps=1, version=\"\")\n",
    "\n",
    "# Save the best model (lowest validation MSE) in the same folder.\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"PS2Q3/{folder_name}\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    monitor=\"val_loss_huber\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "# RichProgressBar gives a nicer progress bar.\n",
    "rich_bar = RichProgressBar()\n",
    "\n",
    "# Instantiate your custom hyperparameter logger callback.\n",
    "hp_logger = HyperparameterLogger()\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss_huber\",\n",
    "    patience=10,\n",
    "    min_delta=0.005,\n",
    "    mode=\"min\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Model Instantiation and Trainer Setup\n",
    "# -----------------------------------------------------------------------------\n",
    "# Assume:\n",
    "#   - continuous_cols is the list of continuous feature names.\n",
    "#   - categorical_cols is the list of categorical feature names.\n",
    "#   - train_df, valid_df, etc. are your preprocessed DataFrames.\n",
    "#   - num_categories: a list with the number of unique categories for each categorical feature.\n",
    "#   - embedding_dims: here we use 1-dimensional embeddings for simplicity.\n",
    "num_categories = [train_df[col].nunique() for col in categorical_cols]\n",
    "embedding_dims = [1] * len(num_categories)\n",
    "\n",
    "# Instantiate the Lightning model.\n",
    "tuned_deep_model_lightning = DeepRegressionLightning(\n",
    "    num_continuous=len(continuous_cols),\n",
    "    num_categories=num_categories,\n",
    "    embedding_dims=embedding_dims,\n",
    "    learning_rate=0.05\n",
    ")\n",
    "\n",
    "# Instantiate the PyTorch Lightning Trainer.\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator='auto',\n",
    "    devices='auto',\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, rich_bar, hp_logger]\n",
    ")\n",
    "\n",
    "# Start training by passing in your DataLoaders.\n",
    "trainer.fit(tuned_deep_model_lightning, train_dataloaders=train_loader, val_dataloaders=valid_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to tuned_deep_reg_preds.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Run predictions using your trained model ---\n",
    "# Make sure your DeepRegressionLightning model (here referred to as model_lightning)\n",
    "# is the trained model. For inference, set the model to evaluation mode.\n",
    "tuned_deep_model_lightning.eval()\n",
    "with torch.no_grad():\n",
    "    preds = tuned_deep_model_lightning(\n",
    "        test_continuous_tensor.to(model_lightning.device),\n",
    "        test_categorical_tensor.to(model_lightning.device)\n",
    "    )\n",
    "    preds = preds.cpu().numpy()  # Convert predictions to a NumPy array\n",
    "\n",
    "# --- Optional Step: Inverse transform predictions if you scaled your target ---\n",
    "# If you used a scaler (e.g., price_scaler) to scale SalePrice during training, apply it here:\n",
    "preds = price_scaler.inverse_transform(preds.reshape(-1, 1)).squeeze()\n",
    "\n",
    "# --- Step 5: Format the predictions into the required CSV format ---\n",
    "output_df = pd.DataFrame({\n",
    "    \"ID\": test_ids,\n",
    "    \"Prediction\": preds\n",
    "})\n",
    "\n",
    "# Optionally, format the predictions to 4 decimal places.\n",
    "output_df[\"Prediction\"] = output_df[\"Prediction\"].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "# --- Step 6: Save the predictions to a CSV file ---\n",
    "output_df.to_csv(\"tuned_deep_reg_preds.csv\", index=False)\n",
    "print(\"Predictions saved to tuned_deep_reg_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HeteroskedasticDeepRegressionWithEmbeddings(nn.Module):\n",
    "    def __init__(self, num_continuous, num_categories, embedding_dims, hidden_units):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_continuous (int): Number of continuous features.\n",
    "            num_categories (list of int): Unique counts for each categorical feature.\n",
    "            embedding_dims (list of int): Embedding dimensions for each categorical feature.\n",
    "            hidden_units (list of int): List of units for each hidden layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Create embedding layers for each categorical feature.\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(num_categories[i], embedding_dims[i]) for i in range(len(num_categories))\n",
    "        ])\n",
    "        total_embedding_dim = sum(embedding_dims)\n",
    "        # Input dimension: continuous features + all embedding dimensions.\n",
    "        input_dim = num_continuous + total_embedding_dim\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for units in hidden_units:\n",
    "            layers.append(nn.Linear(prev_dim, units))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(units))\n",
    "            prev_dim = units\n",
    "        # Final layer outputs 2 values: [mean, log_variance]\n",
    "        layers.append(nn.Linear(prev_dim, 2))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, continuous, categorical):\n",
    "        # Get embeddings for each categorical feature.\n",
    "        embedded = [emb(categorical[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embedded, dim=1)\n",
    "        # Concatenate continuous features with the embeddings.\n",
    "        x = torch.cat([continuous, embedded], dim=1)\n",
    "        out = self.model(x)  # out shape: [batch_size, 2]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class HeteroskedasticRegressionLightning(pl.LightningModule):\n",
    "    def __init__(self, num_continuous, num_categories, embedding_dims, learning_rate=0.05):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_continuous (int): Number of continuous features.\n",
    "            num_categories (list of int): Unique counts for each categorical feature.\n",
    "            embedding_dims (list of int): Embedding dimensions for each categorical feature.\n",
    "            learning_rate (float): Learning rate for the optimizer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()  # Save hyperparameters for logging.\n",
    "        \n",
    "        # Instantiate the heteroskedastic model.\n",
    "        self.model = HeteroskedasticDeepRegressionWithEmbeddings(\n",
    "            num_continuous=num_continuous,\n",
    "            num_categories=num_categories,\n",
    "            embedding_dims=embedding_dims,\n",
    "            hidden_units=[128,64]  # You can adjust the architecture here.\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, continuous, categorical):\n",
    "        # Returns two outputs per instance: [mu, log_variance]\n",
    "        return self.model(continuous, categorical)\n",
    "    \n",
    "    def heteroskedastic_loss(self, preds, target):\n",
    "        # preds is of shape [batch_size, 2]. Split into mu and s (log variance).\n",
    "        mu = preds[:, 0]\n",
    "        s = preds[:, 1]\n",
    "        # Compute the loss as: 0.5 * exp(-s) * (target - mu)^2 + 0.5 * s\n",
    "        loss = 0.5 * torch.exp(-s) * (target - mu) ** 2 + 0.5 * s\n",
    "        return loss.mean()\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        continuous, categorical, target = batch\n",
    "        preds = self(continuous, categorical)\n",
    "        loss = self.heteroskedastic_loss(preds, target)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        continuous, categorical, target = batch\n",
    "        preds = self(continuous, categorical)\n",
    "        loss = self.heteroskedastic_loss(preds, target)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        continuous, categorical, target = batch\n",
    "        preds = self(continuous, categorical)\n",
    "        loss = self.heteroskedastic_loss(preds, target)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=0.0000)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                                        </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model │ HeteroskedasticDeepRegressionWithEmbeddings │ 17.4 K │ train │\n",
       "└───┴───────┴─────────────────────────────────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                                       \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model │ HeteroskedasticDeepRegressionWithEmbeddings │ 17.4 K │ train │\n",
       "└───┴───────┴─────────────────────────────────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 17.4 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 17.4 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 56                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 17.4 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 17.4 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 56                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/trainer/co\n",
       "nnectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider\n",
       "increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/trainer/co\n",
       "nnectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider\n",
       "increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/trainer/co\n",
       "nnectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. \n",
       "Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve \n",
       "performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/trainer/co\n",
       "nnectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. \n",
       "Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve \n",
       "performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/loops/fit_\n",
       "loop.py:298: The number of training batches (15) is smaller than the logging interval \n",
       "Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training \n",
       "epoch.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/nicholasrichards/Desktop/anaconda3/envs/qtm151-f24/lib/python3.12/site-packages/pytorch_lightning/loops/fit_\n",
       "loop.py:298: The number of training batches (15) is smaller than the logging interval \n",
       "Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training \n",
       "epoch.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.076\n",
      "Metric val_loss improved by 0.407 >= min_delta = 0.0. New best score: -0.331\n",
      "Metric val_loss improved by 0.289 >= min_delta = 0.0. New best score: -0.620\n",
      "Metric val_loss improved by 0.152 >= min_delta = 0.0. New best score: -0.772\n",
      "Metric val_loss improved by 0.013 >= min_delta = 0.0. New best score: -0.785\n",
      "Metric val_loss improved by 0.011 >= min_delta = 0.0. New best score: -0.796\n",
      "Monitored metric val_loss did not improve in the last 15 records. Best score: -0.796. Signaling Trainer to stop.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, RichProgressBar, EarlyStopping\n",
    "\n",
    "folder_name = \"HeteroModel1\"\n",
    "\n",
    "# CSVLogger: logs will be saved in \"PS2Q3/HeteroModel1\"\n",
    "logger = CSVLogger(\"PS2Q3\", name=folder_name, flush_logs_every_n_steps=1, version=\"\")\n",
    "\n",
    "# ModelCheckpoint: Monitor the validation heteroskedastic loss.\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"PS2Q3/{folder_name}\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "rich_bar = RichProgressBar()\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=15,\n",
    "    mode=\"min\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# (Optional) If you have a hyperparameter logging callback, include it.\n",
    "hp_logger = HyperparameterLogger() \n",
    "\n",
    "# Instantiate the Lightning model.\n",
    "# Assume continuous_cols and categorical_cols are defined; num_categories and embedding_dims computed as before.\n",
    "num_categories = [train_df[col].nunique() for col in categorical_cols]\n",
    "embedding_dims = [1] * len(num_categories)\n",
    "\n",
    "hetero_deep_lightning = HeteroskedasticRegressionLightning(\n",
    "    num_continuous=len(continuous_cols),\n",
    "    num_categories=num_categories,\n",
    "    embedding_dims=embedding_dims,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "# Instantiate the Trainer.\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator='auto',\n",
    "    devices='auto',\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, rich_bar, early_stopping, hp_logger]\n",
    ")\n",
    "\n",
    "# Start training by passing in your DataLoaders.\n",
    "trainer.fit(hetero_deep_lightning, train_dataloaders=train_loader, val_dataloaders=valid_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our heteroskedactic model prefers a shallower network with moderate hidden values of 128 and 64, no weight decay, a relatively large early stopping patience of 15, and an average learning rate of \n",
    "0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to hetero_deep_preds.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# --- Step 1: Extract IDs from the original test DataFrame ---\n",
    "test_ids = test_df[\"ID\"]\n",
    "\n",
    "# --- Step 2: Prepare the features for prediction ---\n",
    "test_features = test_df.drop(columns=[\"ID\"])\n",
    "test_continuous = test_features[continuous_cols].values\n",
    "test_categorical = test_features[categorical_cols].values\n",
    "\n",
    "# --- Step 3: Convert features to PyTorch tensors ---\n",
    "test_continuous_tensor = torch.tensor(test_continuous, dtype=torch.float32)\n",
    "test_categorical_tensor = torch.tensor(test_categorical, dtype=torch.long)\n",
    "\n",
    "# --- Step 4: Run predictions using your trained heteroskedastic model ---\n",
    "hetero_deep_lightning.eval()\n",
    "with torch.no_grad():\n",
    "    preds = hetero_deep_lightning(\n",
    "        test_continuous_tensor.to(hetero_deep_lightning.device),\n",
    "        test_categorical_tensor.to(hetero_deep_lightning.device)\n",
    "    )\n",
    "    preds = preds.cpu().numpy()  # preds shape is (N, 2)\n",
    "\n",
    "# --- Step 5: Extract the predicted mean (first column) only ---\n",
    "preds_mean = preds[:, 0]\n",
    "\n",
    "# --- Step 6: Inverse-scale the predictions ---\n",
    "# Assuming price_scaler was fitted on the training target.\n",
    "preds_unscaled = price_scaler.inverse_transform(preds_mean.reshape(-1, 1)).squeeze()\n",
    "\n",
    "# --- Step 7: Format the predictions into the required CSV format ---\n",
    "output_df = pd.DataFrame({\n",
    "    \"ID\": test_ids,\n",
    "    \"Prediction\": preds_unscaled\n",
    "})\n",
    "\n",
    "# Format predictions to 4 decimal places.\n",
    "output_df[\"Prediction\"] = output_df[\"Prediction\"].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "# --- Step 8: Save the predictions to a CSV file ---\n",
    "output_df.to_csv(\"hetero_deep_preds.csv\", index=False)\n",
    "print(\"Predictions saved to hetero_deep_preds.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DF shape: (254, 64)\n",
      "ID column shape: (254,)\n",
      "Features shape after dropping ID: (254, 63)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test DF shape:\", test_df.shape)\n",
    "print(\"ID column shape:\", test_df[\"ID\"].shape)\n",
    "print(\"Features shape after dropping ID:\", test_df.drop(columns=[\"ID\"]).shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to hetero_deep_preds.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Step 7: Create output DataFrame in the required format ---\n",
    "output_df = pd.DataFrame({\n",
    "    \"ID\": test_ids,\n",
    "    \"Prediction\": preds_unscaled\n",
    "})\n",
    "\n",
    "# Format predictions to 4 decimal places.\n",
    "output_df[\"Prediction\"] = output_df[\"Prediction\"].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "# --- Step 8: Save the DataFrame to a CSV file ---\n",
    "output_df.to_csv(\"hetero_deep_preds.csv\", index=False)\n",
    "print(\"Predictions saved to hetero_deep_preds.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df shape: (254, 64)\n",
      "ID column shape: (254,)\n",
      "Features shape after dropping ID: (254, 63)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# --- Debug: Check shapes of test DataFrame ---\n",
    "print(\"test_df shape:\", test_df.shape)\n",
    "print(\"ID column shape:\", test_df[\"ID\"].shape)\n",
    "print(\"Features shape after dropping ID:\", test_df.drop(columns=[\"ID\"]).shape)\n",
    "\n",
    "# --- Step 1: Extract IDs from the original test DataFrame ---\n",
    "# Attempt to extract IDs.\n",
    "test_ids = test_df[\"ID\"]\n",
    "\n",
    "# --- Step 2: Prepare features for prediction ---\n",
    "# Drop the \"ID\" column.\n",
    "test_features = test_df.drop(columns=[\"ID\"])\n",
    "\n",
    "# If there's a mismatch in the number of IDs and rows in features, use the DataFrame's index.\n",
    "if len(test_ids) != len(test_features):\n",
    "    print(\"Mismatch detected: using features index as IDs.\")\n",
    "    test_ids = test_features.index\n",
    "\n",
    "# --- Step 3: Extract continuous and categorical features ---\n",
    "test_continuous = test_features[continuous_cols].values\n",
    "test_categorical = test_features[categorical_cols].values\n",
    "\n",
    "# --- Step 4: Convert features to tensors ---\n",
    "test_continuous_tensor = torch.tensor(test_continuous, dtype=torch.float32)\n",
    "test_categorical_tensor = torch.tensor(test_categorical, dtype=torch.long)\n",
    "\n",
    "# --- Step 5: Run predictions using your trained model ---\n",
    "hetero_deep_lightning.eval()\n",
    "with torch.no_grad():\n",
    "    preds = hetero_deep_lightning(\n",
    "        test_continuous_tensor.to(hetero_deep_lightning.device),\n",
    "        test_categorical_tensor.to(hetero_deep_lightning.device)\n",
    "    )\n",
    "    preds = preds.cpu().numpy()\n",
    "\n",
    "# --- Step 6: Inverse-scale the predictions ---\n",
    "# Assuming price_scaler was used during training.\n",
    "preds_unscaled = price_scaler.inverse_transform(preds.reshape(-1, 1)).squeeze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qtm151-f24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
